[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "langchain_openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "find_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "langchain.tools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "format_tool_to_openai_function",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "format_tool_to_openai_function",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "format_tool_to_openai_function",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "format_tool_to_openai_function",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "format_tool_to_openai_function",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "format_tool_to_openai_function",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "format_tool_to_openai_function",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "render_text_description",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "OpenAIFunctionsAgentOutputParser",
        "importPath": "langchain.agents.output_parsers",
        "description": "langchain.agents.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.agents.output_parsers",
        "documentation": {}
    },
    {
        "label": "OpenAIFunctionsAgentOutputParser",
        "importPath": "langchain.agents.output_parsers",
        "description": "langchain.agents.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.agents.output_parsers",
        "documentation": {}
    },
    {
        "label": "OpenAIFunctionsAgentOutputParser",
        "importPath": "langchain.agents.output_parsers",
        "description": "langchain.agents.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.agents.output_parsers",
        "documentation": {}
    },
    {
        "label": "OpenAIFunctionsAgentOutputParser",
        "importPath": "langchain.agents.output_parsers",
        "description": "langchain.agents.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.agents.output_parsers",
        "documentation": {}
    },
    {
        "label": "OpenAIFunctionsAgentOutputParser",
        "importPath": "langchain.agents.output_parsers",
        "description": "langchain.agents.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.agents.output_parsers",
        "documentation": {}
    },
    {
        "label": "OpenAIFunctionsAgentOutputParser",
        "importPath": "langchain.agents.output_parsers",
        "description": "langchain.agents.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.agents.output_parsers",
        "documentation": {}
    },
    {
        "label": "OpenAIFunctionsAgentOutputParser",
        "importPath": "langchain.agents.output_parsers",
        "description": "langchain.agents.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.agents.output_parsers",
        "documentation": {}
    },
    {
        "label": "ReActSingleInputOutputParser",
        "importPath": "langchain.agents.output_parsers",
        "description": "langchain.agents.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.agents.output_parsers",
        "documentation": {}
    },
    {
        "label": "format_to_openai_functions",
        "importPath": "langchain.agents.format_scratchpad",
        "description": "langchain.agents.format_scratchpad",
        "isExtraImport": true,
        "detail": "langchain.agents.format_scratchpad",
        "documentation": {}
    },
    {
        "label": "format_to_openai_functions",
        "importPath": "langchain.agents.format_scratchpad",
        "description": "langchain.agents.format_scratchpad",
        "isExtraImport": true,
        "detail": "langchain.agents.format_scratchpad",
        "documentation": {}
    },
    {
        "label": "format_to_openai_functions",
        "importPath": "langchain.agents.format_scratchpad",
        "description": "langchain.agents.format_scratchpad",
        "isExtraImport": true,
        "detail": "langchain.agents.format_scratchpad",
        "documentation": {}
    },
    {
        "label": "format_to_openai_functions",
        "importPath": "langchain.agents.format_scratchpad",
        "description": "langchain.agents.format_scratchpad",
        "isExtraImport": true,
        "detail": "langchain.agents.format_scratchpad",
        "documentation": {}
    },
    {
        "label": "format_to_openai_functions",
        "importPath": "langchain.agents.format_scratchpad",
        "description": "langchain.agents.format_scratchpad",
        "isExtraImport": true,
        "detail": "langchain.agents.format_scratchpad",
        "documentation": {}
    },
    {
        "label": "format_to_openai_functions",
        "importPath": "langchain.agents.format_scratchpad",
        "description": "langchain.agents.format_scratchpad",
        "isExtraImport": true,
        "detail": "langchain.agents.format_scratchpad",
        "documentation": {}
    },
    {
        "label": "format_log_to_str",
        "importPath": "langchain.agents.format_scratchpad",
        "description": "langchain.agents.format_scratchpad",
        "isExtraImport": true,
        "detail": "langchain.agents.format_scratchpad",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnableMap",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnableLambda",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "create_tool_calling_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "param",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "param",
        "description": "param",
        "detail": "param",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "wikipedia",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wikipedia",
        "description": "wikipedia",
        "detail": "wikipedia",
        "documentation": {}
    },
    {
        "label": "panel",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "panel",
        "description": "panel",
        "detail": "panel",
        "documentation": {}
    },
    {
        "label": "mlflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mlflow",
        "description": "mlflow",
        "detail": "mlflow",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "whylogs",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "whylogs",
        "description": "whylogs",
        "detail": "whylogs",
        "documentation": {}
    },
    {
        "label": "llm_metrics",
        "importPath": "langkit",
        "description": "langkit",
        "isExtraImport": true,
        "detail": "langkit",
        "documentation": {}
    },
    {
        "label": "llm_metrics",
        "importPath": "langkit",
        "description": "langkit",
        "isExtraImport": true,
        "detail": "langkit",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "convert_to_openai_function",
        "importPath": "langchain_core.utils.function_calling",
        "description": "langchain_core.utils.function_calling",
        "isExtraImport": true,
        "detail": "langchain_core.utils.function_calling",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema.output_parser",
        "description": "langchain.schema.output_parser",
        "isExtraImport": true,
        "detail": "langchain.schema.output_parser",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema.output_parser",
        "description": "langchain.schema.output_parser",
        "isExtraImport": true,
        "detail": "langchain.schema.output_parser",
        "documentation": {}
    },
    {
        "label": "DocArrayInMemorySearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "convert_pydantic_to_openai_function",
        "importPath": "langchain.utils.openai_functions",
        "description": "langchain.utils.openai_functions",
        "isExtraImport": true,
        "detail": "langchain.utils.openai_functions",
        "documentation": {}
    },
    {
        "label": "convert_pydantic_to_openai_function",
        "importPath": "langchain.utils.openai_functions",
        "description": "langchain.utils.openai_functions",
        "isExtraImport": true,
        "detail": "langchain.utils.openai_functions",
        "documentation": {}
    },
    {
        "label": "JsonOutputFunctionsParser",
        "importPath": "langchain.output_parsers.openai_functions",
        "description": "langchain.output_parsers.openai_functions",
        "isExtraImport": true,
        "detail": "langchain.output_parsers.openai_functions",
        "documentation": {}
    },
    {
        "label": "JsonKeyOutputFunctionsParser",
        "importPath": "langchain.output_parsers.openai_functions",
        "description": "langchain.output_parsers.openai_functions",
        "isExtraImport": true,
        "detail": "langchain.output_parsers.openai_functions",
        "documentation": {}
    },
    {
        "label": "WebBaseLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "openapi_spec_to_openai_fn",
        "importPath": "langchain.chains.openai_functions.openapi",
        "description": "langchain.chains.openai_functions.openapi",
        "isExtraImport": true,
        "detail": "langchain.chains.openai_functions.openapi",
        "documentation": {}
    },
    {
        "label": "OpenAPISpec",
        "importPath": "langchain.utilities.openapi",
        "description": "langchain.utilities.openapi",
        "isExtraImport": true,
        "detail": "langchain.utilities.openapi",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema.agent",
        "description": "langchain.schema.agent",
        "isExtraImport": true,
        "detail": "langchain.schema.agent",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema.agent",
        "description": "langchain.schema.agent",
        "isExtraImport": true,
        "detail": "langchain.schema.agent",
        "documentation": {}
    },
    {
        "label": "uvicorn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uvicorn",
        "description": "uvicorn",
        "detail": "uvicorn",
        "documentation": {}
    },
    {
        "label": "langkit.toxicity",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langkit.toxicity",
        "description": "langkit.toxicity",
        "detail": "langkit.toxicity",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "sysconfig",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sysconfig",
        "description": "sysconfig",
        "detail": "sysconfig",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "winreg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "winreg",
        "description": "winreg",
        "detail": "winreg",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "BaseCallbackHandler",
        "importPath": "langchain.callbacks.base",
        "description": "langchain.callbacks.base",
        "isExtraImport": true,
        "detail": "langchain.callbacks.base",
        "documentation": {}
    },
    {
        "label": "LLMResult",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "create_csv_agent",
        "importPath": "langchain_experimental.agents.agent_toolkits",
        "description": "langchain_experimental.agents.agent_toolkits",
        "isExtraImport": true,
        "detail": "langchain_experimental.agents.agent_toolkits",
        "documentation": {}
    },
    {
        "label": "create_csv_agent",
        "importPath": "langchain_experimental.agents.agent_toolkits",
        "description": "langchain_experimental.agents.agent_toolkits",
        "isExtraImport": true,
        "detail": "langchain_experimental.agents.agent_toolkits",
        "documentation": {}
    },
    {
        "label": "create_csv_agent",
        "importPath": "langchain_experimental.agents.agent_toolkits",
        "description": "langchain_experimental.agents.agent_toolkits",
        "isExtraImport": true,
        "detail": "langchain_experimental.agents.agent_toolkits",
        "documentation": {}
    },
    {
        "label": "hub",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "hub",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "hub",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "TavilySearchResults",
        "importPath": "langchain_community.tools.tavily_search",
        "description": "langchain_community.tools.tavily_search",
        "isExtraImport": true,
        "detail": "langchain_community.tools.tavily_search",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain_core.tools",
        "description": "langchain_core.tools",
        "isExtraImport": true,
        "detail": "langchain_core.tools",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain_core.tools",
        "description": "langchain_core.tools",
        "isExtraImport": true,
        "detail": "langchain_core.tools",
        "documentation": {}
    },
    {
        "label": "ChatAnthropic",
        "importPath": "langchain_anthropic",
        "description": "langchain_anthropic",
        "isExtraImport": true,
        "detail": "langchain_anthropic",
        "documentation": {}
    },
    {
        "label": "AgentCallbackHandler",
        "importPath": "callbacks",
        "description": "callbacks",
        "isExtraImport": true,
        "detail": "callbacks",
        "documentation": {}
    },
    {
        "label": "PythonREPLTool",
        "importPath": "langchain_experimental.tools",
        "description": "langchain_experimental.tools",
        "isExtraImport": true,
        "detail": "langchain_experimental.tools",
        "documentation": {}
    },
    {
        "label": "PythonREPLTool",
        "importPath": "langchain_experimental.tools",
        "description": "langchain_experimental.tools",
        "isExtraImport": true,
        "detail": "langchain_experimental.tools",
        "documentation": {}
    },
    {
        "label": "OpenMeteoInput",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "class OpenMeteoInput(BaseModel):\n    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n@tool(args_schema=OpenMeteoInput)\n# @traceable  # Auto-trace this function\ndef get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "cbfs",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "class cbfs(param.Parameterized):\n    def __init__(self, tools, **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []\n        self.functions = [format_tool_to_openai_function(f) for f in tools]\n        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n        self.prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are helpful but sassy assistant\"),\n            MessagesPlaceholder(variable_name=\"chat_history\"),",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "get_current_temperature",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "def get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {\n        'latitude': latitude,\n        'longitude': longitude,\n        'hourly': 'temperature_2m',\n        'forecast_days': 1,\n    }",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "search_wikipedia",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "def search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n    for page_title in page_titles[: 3]:\n        try:\n            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n        except (wikipedia.exceptions.PageError, wikipedia.exceptions.DisambiguationError):\n            pass",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "create_your_own",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "def create_your_own(query: str) -> str:\n    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n    print(type(query))\n    return query[::-1]\n#----------------------------------------------------------------\nclass cbfs(param.Parameterized):\n    def __init__(self, tools, **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []\n        self.functions = [format_tool_to_openai_function(f) for f in tools]",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "tools = [get_current_temperature, search_wikipedia, create_your_own]\nimport panel as pn  # GUI\npn.extension()\ncb = cbfs(tools)\ninp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "cb",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "cb = cbfs(tools)\ninp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "inp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "conversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "tab1",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "tab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "dashboard",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "description": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "peekOfCode": "dashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)\n# print(dashboard)\n# Start the Panel server to render the chatbot dashboard\ndashboard.show(port=5010)  # You can specify any port number you prefer\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-langsmith",
        "documentation": {}
    },
    {
        "label": "OpenMeteoInput",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "class OpenMeteoInput(BaseModel):\n    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n@tool(args_schema=OpenMeteoInput)\ndef get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {\n        'latitude': latitude,",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "cbfs",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "class cbfs(param.Parameterized):\n    def __init__(self, tools, **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []\n        self.functions = [format_tool_to_openai_function(f) for f in tools]\n        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n        self.prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are helpful but sassy assistant\"),\n            MessagesPlaceholder(variable_name=\"chat_history\"),",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "get_current_temperature",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "def get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {\n        'latitude': latitude,\n        'longitude': longitude,\n        'hourly': 'temperature_2m',\n        'forecast_days': 1,\n    }",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "search_wikipedia",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "def search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    with mlflow.start_run(run_name=\"Wikipedia_Search\", nested=True):\n        try:\n            page_titles = wikipedia.search(query)\n            summaries = []\n            for page_title in page_titles[: 3]:\n                try:\n                    wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n                    summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "create_your_own",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "def create_your_own(query: str) -> str:\n    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n    print(type(query))\n    return query[::-1]\n#----------------------------------------------------------------\nclass cbfs(param.Parameterized):\n    def __init__(self, tools, **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []\n        self.functions = [format_tool_to_openai_function(f) for f in tools]",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "tools = [get_current_temperature, search_wikipedia, create_your_own]\npn.extension()\ncb = cbfs(tools)\ninp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "cb",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "cb = cbfs(tools)\ninp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "inp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "conversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "tab1",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "tab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "dashboard",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "description": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "peekOfCode": "dashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)\n# print(dashboard)\n# Start the Panel server to render the chatbot dashboard\ndashboard.show(port=5015)  # You can specify any port number you prefer\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-mlflow",
        "documentation": {}
    },
    {
        "label": "OpenMeteoInput",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "class OpenMeteoInput(BaseModel):\n    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n@tool(args_schema=OpenMeteoInput)\ndef get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # # Log the input data\n    # with logger.log():\n    #     logger.log_dataframe(pd.DataFrame([{\"latitude\": latitude, \"longitude\": longitude}]))",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "cbfs",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "class cbfs(param.Parameterized):\n    def __init__(self, tools, **params):\n        super(cbfs, self).__init__(**params)\n        self.panels = []\n        self.functions = [format_tool_to_openai_function(f) for f in tools]\n        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n        self.prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a helpful but sassy assistant\"),\n            MessagesPlaceholder(variable_name=\"chat_history\"),",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "get_current_temperature",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "def get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # # Log the input data\n    # with logger.log():\n    #     logger.log_dataframe(pd.DataFrame([{\"latitude\": latitude, \"longitude\": longitude}]))\n    # Parameters for the request\n    params = {\n        'latitude': latitude,\n        'longitude': longitude,",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "search_wikipedia",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "def search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    # # Log the input query\n    # with logger.log():\n    #     logger.log_dataframe(pd.DataFrame([{\"query\": query}]))\n    page_titles = wikipedia.search(query)\n    summaries = []\n    for page_title in page_titles[: 3]:\n        try:\n            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "create_your_own",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "def create_your_own(query: str) -> str:\n    \"\"\"Example custom tool function.\"\"\"\n    # # Log the input query\n    # with logger.log():\n    #     logger.log_dataframe(pd.DataFrame([{\"query\": query}]))\n    result = query[::-1]  # Reverse the string for illustration\n    # # Log the output\n    # with logger.log():\n    #     logger.log_dataframe(pd.DataFrame([{\"output\": result}]))\n    return result",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "tools = [get_current_temperature, search_wikipedia, create_your_own]\n# GUI Setup\npn.extension()\ncb = cbfs(tools)\ninp = pn.widgets.TextInput(placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation, loading_indicator=True, height=400),",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "cb",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "cb = cbfs(tools)\ninp = pn.widgets.TextInput(placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation, loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "inp = pn.widgets.TextInput(placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation, loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "conversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation, loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "tab1",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "tab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation, loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "dashboard",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "description": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "peekOfCode": "dashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)\n# Start the Panel server to render the chatbot dashboard\ndashboard.show(port=5020)\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.Create_chatbot-pipeline-whylog",
        "documentation": {}
    },
    {
        "label": "OpenMeteoInput",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "class OpenMeteoInput(BaseModel):\n    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n@tool(args_schema=OpenMeteoInput)\ndef get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {\n        'latitude': latitude,",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "cbfs",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "class cbfs(param.Parameterized):\n    def __init__(self, tools, **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []\n        self.functions = [format_tool_to_openai_function(f) for f in tools]\n        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n        self.prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are helpful but sassy assistant\"),\n            MessagesPlaceholder(variable_name=\"chat_history\"),",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "get_current_temperature",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "def get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {\n        'latitude': latitude,\n        'longitude': longitude,\n        'hourly': 'temperature_2m',\n        'forecast_days': 1,\n    }",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "search_wikipedia",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "def search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n    for page_title in page_titles[: 3]:\n        try:\n            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n        except (\n            self.wiki_client.exceptions.PageError,",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "create_your_own",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "def create_your_own(query: str) -> str:\n    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n    print(type(query))\n    return query[::-1]\ntools = [get_current_temperature, search_wikipedia, create_your_own]\nimport panel as pn  # GUI\npn.extension()\n# import panel as pn\nimport param\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "tools = [get_current_temperature, search_wikipedia, create_your_own]\nimport panel as pn  # GUI\npn.extension()\n# import panel as pn\nimport param\n#----------------------------------------------------------------\nclass cbfs(param.Parameterized):\n    def __init__(self, tools, **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "cb",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "cb = cbfs(tools)\ninp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "inp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "conversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "tab1",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "tab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "dashboard",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot",
        "description": "Functions_Langchain.Create_chatbot",
        "peekOfCode": "dashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)\n# print(dashboard)\n# Start the Panel server to render the chatbot dashboard\ndashboard.show(port=5010)  # You can specify any port number you prefer\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.Create_chatbot",
        "documentation": {}
    },
    {
        "label": "OpenMeteoInput",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot_fastapi",
        "description": "Functions_Langchain.Create_chatbot_fastapi",
        "peekOfCode": "class OpenMeteoInput(BaseModel):\n    latitude: float\n    longitude: float\n# FastAPI route for fetching the temperature\n@app.post(\"/get_current_temperature\")\ndef get_current_temperature(data: OpenMeteoInput):\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    params = {\n        'latitude': data.latitude,",
        "detail": "Functions_Langchain.Create_chatbot_fastapi",
        "documentation": {}
    },
    {
        "label": "ChatInput",
        "kind": 6,
        "importPath": "Functions_Langchain.Create_chatbot_fastapi",
        "description": "Functions_Langchain.Create_chatbot_fastapi",
        "peekOfCode": "class ChatInput(BaseModel):\n    query: str\n# tools = [get_current_temperature, search_wikipedia, create_your_own]\n# Convert functions to OpenAI-compatible tools\ntools = [\n    convert_to_openai_function(get_current_temperature),\n    convert_to_openai_function(search_wikipedia),\n    convert_to_openai_function(create_your_own)\n]\n@app.post(\"/chatbot\")",
        "detail": "Functions_Langchain.Create_chatbot_fastapi",
        "documentation": {}
    },
    {
        "label": "get_current_temperature",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot_fastapi",
        "description": "Functions_Langchain.Create_chatbot_fastapi",
        "peekOfCode": "def get_current_temperature(data: OpenMeteoInput):\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    params = {\n        'latitude': data.latitude,\n        'longitude': data.longitude,\n        'hourly': 'temperature_2m',\n        'forecast_days': 1,\n    }\n    response = requests.get(BASE_URL, params=params)",
        "detail": "Functions_Langchain.Create_chatbot_fastapi",
        "documentation": {}
    },
    {
        "label": "search_wikipedia",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot_fastapi",
        "description": "Functions_Langchain.Create_chatbot_fastapi",
        "peekOfCode": "def search_wikipedia(query: str):\n    \"\"\"Search Wikipedia and return page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n    for page_title in page_titles[:3]:\n        try:\n            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)\n            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n        except (wikipedia.exceptions.PageError, wikipedia.exceptions.DisambiguationError):\n            continue",
        "detail": "Functions_Langchain.Create_chatbot_fastapi",
        "documentation": {}
    },
    {
        "label": "create_your_own",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot_fastapi",
        "description": "Functions_Langchain.Create_chatbot_fastapi",
        "peekOfCode": "def create_your_own(query: str):\n    \"\"\"Reverse the string provided.\"\"\"\n    return {\"reversed_query\": query[::-1]}\n# Create chatbot functionality with FastAPI POST route\nclass ChatInput(BaseModel):\n    query: str\n# tools = [get_current_temperature, search_wikipedia, create_your_own]\n# Convert functions to OpenAI-compatible tools\ntools = [\n    convert_to_openai_function(get_current_temperature),",
        "detail": "Functions_Langchain.Create_chatbot_fastapi",
        "documentation": {}
    },
    {
        "label": "chatbot",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot_fastapi",
        "description": "Functions_Langchain.Create_chatbot_fastapi",
        "peekOfCode": "def chatbot(query: ChatInput):\n    \"\"\"Handle chatbot queries.\"\"\"\n    # model = ChatOpenAI(temperature=0).bind(functions=[format_tool_to_openai_function(f) for f in tools])\n        # Use the new function `convert_to_openai_function()`\n    functions = [convert_to_openai_function(f) for f in tools]\n    model = ChatOpenAI(temperature=0).bind(functions=functions)\n    # memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n    # Update: Memory usage as per new version\n    memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"input\", return_messages=True)\n    prompt = ChatPromptTemplate.from_messages([",
        "detail": "Functions_Langchain.Create_chatbot_fastapi",
        "documentation": {}
    },
    {
        "label": "read_root",
        "kind": 2,
        "importPath": "Functions_Langchain.Create_chatbot_fastapi",
        "description": "Functions_Langchain.Create_chatbot_fastapi",
        "peekOfCode": "def read_root():\n    return {\"message\": \"Welcome to the FastAPI Chatbot API!\"}",
        "detail": "Functions_Langchain.Create_chatbot_fastapi",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot_fastapi",
        "description": "Functions_Langchain.Create_chatbot_fastapi",
        "peekOfCode": "app = FastAPI()\n# Define input schema for the temperature API\nclass OpenMeteoInput(BaseModel):\n    latitude: float\n    longitude: float\n# FastAPI route for fetching the temperature\n@app.post(\"/get_current_temperature\")\ndef get_current_temperature(data: OpenMeteoInput):\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"",
        "detail": "Functions_Langchain.Create_chatbot_fastapi",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "Functions_Langchain.Create_chatbot_fastapi",
        "description": "Functions_Langchain.Create_chatbot_fastapi",
        "peekOfCode": "tools = [\n    convert_to_openai_function(get_current_temperature),\n    convert_to_openai_function(search_wikipedia),\n    convert_to_openai_function(create_your_own)\n]\n@app.post(\"/chatbot\")\ndef chatbot(query: ChatInput):\n    \"\"\"Handle chatbot queries.\"\"\"\n    # model = ChatOpenAI(temperature=0).bind(functions=[format_tool_to_openai_function(f) for f in tools])\n        # Use the new function `convert_to_openai_function()`",
        "detail": "Functions_Langchain.Create_chatbot_fastapi",
        "documentation": {}
    },
    {
        "label": "get_current_weather",
        "kind": 2,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "def get_current_weather(location: str, unit: str = \"fahrenheit\") -> json:\n    \"\"\"Get the current weather in the given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"unit\": unit,\n        \"temperature\": 72,\n        \"forcast\": [\"sunny\", \"windy\"]        \n    }\n    return json.dumps(weather_info)\n# # print(get_current_weather(\"kolkata\"))",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "functions = [\n    {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in the given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"},\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"default\": \"fahrenheit\"}\n        },",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"what is the weather in Kolkata?\"\n    }\n]\nfrom openai import OpenAI\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\nresponse = client.chat.completions.create(\n    model = \"gpt-4\",",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\nresponse = client.chat.completions.create(\n    model = \"gpt-4\",\n    # model = \"gpt-3.5-turbo-0613\",\n    messages = messages,\n    functions = functions\n)\nprint(response)\narguments_str = response.choices[0].message.function_call.arguments\nargs = json.loads(arguments_str)",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "response = client.chat.completions.create(\n    model = \"gpt-4\",\n    # model = \"gpt-3.5-turbo-0613\",\n    messages = messages,\n    functions = functions\n)\nprint(response)\narguments_str = response.choices[0].message.function_call.arguments\nargs = json.loads(arguments_str)\nprint(get_current_weather(args))",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "arguments_str",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "arguments_str = response.choices[0].message.function_call.arguments\nargs = json.loads(arguments_str)\nprint(get_current_weather(args))\n#----------------------------------------------------------------\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"hi\",\n    }\n]",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "args = json.loads(arguments_str)\nprint(get_current_weather(args))\n#----------------------------------------------------------------\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"hi\",\n    }\n]\nresponse_1 = client.chat.completions.create(",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"hi\",\n    }\n]\nresponse_1 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,\n    functions = functions",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "response_1",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "response_1 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,\n    functions = functions\n)\nprint(response_1)\n#----------------------------------------------------------------\nresponse_2 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "response_2",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "response_2 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,\n    functions = functions,\n    function_call=\"auto\",\n)\nprint(response_2)\n#----------------------------------------------------------------\nresponse_3 = client.chat.completions.create(\n    model = \"gpt-4\",",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "response_3",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "response_3 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,\n    functions = functions,\n    function_call=\"none\",\n)\nprint(response_3)\n#----------------------------------------------------------------\nmessages = [\n    {",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What's the weather in Boston?\",\n    }\n]\nresponse_4 = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=messages,\n    functions=functions,",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "response_4",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "response_4 = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=messages,\n    functions=functions,\n    function_call=\"none\",\n)\nprint(response_4)\n#----------------------------------------------------------------\nmessages = [\n    {",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"hi\",\n    }\n]\nresponse_5 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,\n    functions = functions,",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "response_5",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "response_5 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,\n    functions = functions,\n    function_call={\"name\": \"get_current_weather\"},\n)\nprint(response_5)\n#----------------------------------------------------------------\nmessages = [\n    {",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What's the weather like in Boston!\",\n    }\n]\nresponse_5 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,\n    functions = functions,",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "response_5",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "response_5 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,\n    functions = functions,\n    function_call={\"name\": \"get_current_weather\"},\n)\nprint(response_5)\n#----------------------------------------------------------------\nmessages.append(response_5.choices[0].message)\nargs = json.loads(response_5.choices[0].message.function_call.arguments)",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "args = json.loads(response_5.choices[0].message.function_call.arguments)\nobservation = get_current_weather(args)\nmessages.append(\n        {\n            \"role\": \"function\",\n            \"name\": \"get_current_weather\",\n            \"content\": observation,\n        }\n)\nresponse_6 = client.chat.completions.create(",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "observation",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "observation = get_current_weather(args)\nmessages.append(\n        {\n            \"role\": \"function\",\n            \"name\": \"get_current_weather\",\n            \"content\": observation,\n        }\n)\nresponse_6 = client.chat.completions.create(\n    model = \"gpt-4\",",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "response_6",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "response_6 = client.chat.completions.create(\n    model = \"gpt-4\",\n    messages = messages,\n    # functions = functions,\n    # function_call={\"name\": \"get_current_weather\"},\n)\nprint(response_6)\n#----------------------------------------------------------------\ntool_functions = [\n    {",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "tool_functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "tool_functions = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in the given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"},",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"what is the weather in Kolkata?\"\n    }\n]\nresp_11 = openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    tools=tool_functions,",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "resp_11",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "resp_11 = openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    tools=tool_functions,\n    )\nprint(f\"\\n \\n FUNCTION CALLING USING LATEST TOOLS : \\n \\n {resp_11}\")\n# arguments_str_11 = resp_11.choices[0].message.function_call.arguments\n# args_11 = json.loads(arguments_str_11)\ntool_call = resp_11.choices[0].message.tool_calls[0]\nargs_11 = json.loads(tool_call.function.arguments)",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "tool_call",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "tool_call = resp_11.choices[0].message.tool_calls[0]\nargs_11 = json.loads(tool_call.function.arguments)\n# args_11 = json.loads(tool_call['function']['arguments'])\nprint(get_current_weather(args_11))\n#----------------------------------------------------------------\nmessages_hi = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hi !\"\n    }",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "args_11",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "args_11 = json.loads(tool_call.function.arguments)\n# args_11 = json.loads(tool_call['function']['arguments'])\nprint(get_current_weather(args_11))\n#----------------------------------------------------------------\nmessages_hi = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hi !\"\n    }\n]",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "messages_hi",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "messages_hi = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hi !\"\n    }\n]\nresp_111 = openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages_hi,\n    tools=tool_functions,",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "resp_111",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "resp_111 = openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages_hi,\n    tools=tool_functions,\n    )\n# args_111 = resp_111.choices[0].message.tool_calls[0].function.arguments\nargs_111 = resp_111.choices[0].message.content\nargs_111 = json.loads(args_111)\nprint(args_111)\nprint(get_current_weather(args_111))",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "args_111",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "args_111 = resp_111.choices[0].message.content\nargs_111 = json.loads(args_111)\nprint(args_111)\nprint(get_current_weather(args_111))",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "args_111",
        "kind": 5,
        "importPath": "Functions_Langchain.L1-openai_functions_student",
        "description": "Functions_Langchain.L1-openai_functions_student",
        "peekOfCode": "args_111 = json.loads(args_111)\nprint(args_111)\nprint(get_current_weather(args_111))",
        "detail": "Functions_Langchain.L1-openai_functions_student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\nprint(chain.invoke({\"topic\": \"Bengali\"}))\n#----------------------------------------------------------------\n# from langchain.embeddings import OpenAIEmbeddings\nfrom langchain_openai import OpenAIEmbeddings\n# from langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain_community.vectorstores import DocArrayInMemorySearch",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "model = ChatOpenAI()\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\nprint(chain.invoke({\"topic\": \"Bengali\"}))\n#----------------------------------------------------------------\n# from langchain.embeddings import OpenAIEmbeddings\nfrom langchain_openai import OpenAIEmbeddings\n# from langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\nvectorstore = DocArrayInMemorySearch.from_texts(",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "output_parser",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "output_parser = StrOutputParser()\nchain = prompt | model | output_parser\nprint(chain.invoke({\"topic\": \"Bengali\"}))\n#----------------------------------------------------------------\n# from langchain.embeddings import OpenAIEmbeddings\nfrom langchain_openai import OpenAIEmbeddings\n# from langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\nvectorstore = DocArrayInMemorySearch.from_texts(\n    [\"Soumya works in IBM at Kolkata\", \"Bears like to eat honey\"],",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "chain = prompt | model | output_parser\nprint(chain.invoke({\"topic\": \"Bengali\"}))\n#----------------------------------------------------------------\n# from langchain.embeddings import OpenAIEmbeddings\nfrom langchain_openai import OpenAIEmbeddings\n# from langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\nvectorstore = DocArrayInMemorySearch.from_texts(\n    [\"Soumya works in IBM at Kolkata\", \"Bears like to eat honey\"],\n    embedding=OpenAIEmbeddings(),",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "vectorstore = DocArrayInMemorySearch.from_texts(\n    [\"Soumya works in IBM at Kolkata\", \"Bears like to eat honey\"],\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\nsoumya = retriever.get_relevant_documents(\"where does Soumya work?\")\nprint(soumya)\nbear = retriever.get_relevant_documents(\"what bears eat?\")\nprint(bear)\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "retriever = vectorstore.as_retriever()\nsoumya = retriever.get_relevant_documents(\"where does Soumya work?\")\nprint(soumya)\nbear = retriever.get_relevant_documents(\"what bears eat?\")\nprint(bear)\n#----------------------------------------------------------------\ntemplate = \"\"\" Answer the question based only on following context:\n{context}\nQuestion: {question}\n\"\"\"",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "soumya",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "soumya = retriever.get_relevant_documents(\"where does Soumya work?\")\nprint(soumya)\nbear = retriever.get_relevant_documents(\"what bears eat?\")\nprint(bear)\n#----------------------------------------------------------------\ntemplate = \"\"\" Answer the question based only on following context:\n{context}\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "bear",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "bear = retriever.get_relevant_documents(\"what bears eat?\")\nprint(bear)\n#----------------------------------------------------------------\ntemplate = \"\"\" Answer the question based only on following context:\n{context}\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nfrom langchain.schema.runnable import RunnableMap\nchain = RunnableMap({",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "template = \"\"\" Answer the question based only on following context:\n{context}\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nfrom langchain.schema.runnable import RunnableMap\nchain = RunnableMap({\n        \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n        \"question\": lambda x: x[\"question\"]                                                              \n    }) | prompt | model | output_parser",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(template)\nfrom langchain.schema.runnable import RunnableMap\nchain = RunnableMap({\n        \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n        \"question\": lambda x: x[\"question\"]                                                              \n    }) | prompt | model | output_parser\nanswer = chain.invoke({\"question\": \"Where does Soumya work?\"})\nprint(answer)\n#----------------------------------------------------------------\ninputs = RunnableMap({",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "chain = RunnableMap({\n        \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n        \"question\": lambda x: x[\"question\"]                                                              \n    }) | prompt | model | output_parser\nanswer = chain.invoke({\"question\": \"Where does Soumya work?\"})\nprint(answer)\n#----------------------------------------------------------------\ninputs = RunnableMap({\n    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n    \"question\": lambda x: x[\"question\"]",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "answer",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "answer = chain.invoke({\"question\": \"Where does Soumya work?\"})\nprint(answer)\n#----------------------------------------------------------------\ninputs = RunnableMap({\n    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n    \"question\": lambda x: x[\"question\"]\n})\nip_s = inputs.invoke({\"question\": \"where did Soumya work?\"})\nprint(ip_s)\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "inputs = RunnableMap({\n    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n    \"question\": lambda x: x[\"question\"]\n})\nip_s = inputs.invoke({\"question\": \"where did Soumya work?\"})\nprint(ip_s)\n#----------------------------------------------------------------\nfunctions = [\n    {\n        \"name\": \"weather_search\",",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "ip_s",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "ip_s = inputs.invoke({\"question\": \"where did Soumya work?\"})\nprint(ip_s)\n#----------------------------------------------------------------\nfunctions = [\n    {\n        \"name\": \"weather_search\",\n        \"description\": \"Search for weather given a airport code\",\n        \"parameters\":{\n            \"type\": \"object\",\n        \"properties\": {",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "functions = [\n    {\n        \"name\": \"weather_search\",\n        \"description\": \"Search for weather given a airport code\",\n        \"parameters\":{\n            \"type\": \"object\",\n        \"properties\": {\n          \"airport_code\": {\n            \"type\": \"string\",\n            \"description\": \"The airport code to get the weather for\"",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"human\", \"{input}\")\n    ]\n)\nmodel = ChatOpenAI(temperature=0).bind(functions=functions)\nrunnable = prompt | model\nanswer_ac = runnable.invoke({\"input\": \"What is the weather in SF?\"})\nprint(f\"\\n Airport Weather Updates : /n {answer_ac}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "model = ChatOpenAI(temperature=0).bind(functions=functions)\nrunnable = prompt | model\nanswer_ac = runnable.invoke({\"input\": \"What is the weather in SF?\"})\nprint(f\"\\n Airport Weather Updates : /n {answer_ac}\")\n#----------------------------------------------------------------\nfunctions = [\n    {\n      \"name\": \"weather_search\",\n      \"description\": \"Search for weather given an airport code\",\n      \"parameters\": {",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "runnable",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "runnable = prompt | model\nanswer_ac = runnable.invoke({\"input\": \"What is the weather in SF?\"})\nprint(f\"\\n Airport Weather Updates : /n {answer_ac}\")\n#----------------------------------------------------------------\nfunctions = [\n    {\n      \"name\": \"weather_search\",\n      \"description\": \"Search for weather given an airport code\",\n      \"parameters\": {\n        \"type\": \"object\",",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "answer_ac",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "answer_ac = runnable.invoke({\"input\": \"What is the weather in SF?\"})\nprint(f\"\\n Airport Weather Updates : /n {answer_ac}\")\n#----------------------------------------------------------------\nfunctions = [\n    {\n      \"name\": \"weather_search\",\n      \"description\": \"Search for weather given an airport code\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "functions = [\n    {\n      \"name\": \"weather_search\",\n      \"description\": \"Search for weather given an airport code\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"airport_code\": {\n            \"type\": \"string\",\n            \"description\": \"The airport code to get the weather for\"",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "model = model.bind(functions=functions)\nrunnable = prompt | model\nanswer_both = runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})\nprint(f\"\\n Airport Weather Updates : /n {answer_both}\")\n#----------------------------------------------------------------\nfrom langchain.llms import OpenAI\nimport json\nsimple_model = OpenAI(\n    temperature=0, \n    max_tokens=1000, ",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "runnable",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "runnable = prompt | model\nanswer_both = runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})\nprint(f\"\\n Airport Weather Updates : /n {answer_both}\")\n#----------------------------------------------------------------\nfrom langchain.llms import OpenAI\nimport json\nsimple_model = OpenAI(\n    temperature=0, \n    max_tokens=1000, \n    model=\"gpt-3.5-turbo-instruct\"",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "answer_both",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "answer_both = runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})\nprint(f\"\\n Airport Weather Updates : /n {answer_both}\")\n#----------------------------------------------------------------\nfrom langchain.llms import OpenAI\nimport json\nsimple_model = OpenAI(\n    temperature=0, \n    max_tokens=1000, \n    model=\"gpt-3.5-turbo-instruct\"\n)",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "simple_model",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "simple_model = OpenAI(\n    temperature=0, \n    max_tokens=1000, \n    model=\"gpt-3.5-turbo-instruct\"\n)\nsimple_chain = simple_model | json.loads\nchallenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\nsimple_model.invoke(challenge)\n#----------------------------------------------------------------\n# simple_chain.invoke(challenge)",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "simple_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "simple_chain = simple_model | json.loads\nchallenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\nsimple_model.invoke(challenge)\n#----------------------------------------------------------------\n# simple_chain.invoke(challenge)\nmodel = ChatOpenAI(temperature=0)\nchain = model | StrOutputParser() | json.loads\nchain.invoke(challenge)\nfinal_chain = simple_chain.with_fallbacks([chain])\nfinal_chain.invoke(challenge)",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "challenge",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\nsimple_model.invoke(challenge)\n#----------------------------------------------------------------\n# simple_chain.invoke(challenge)\nmodel = ChatOpenAI(temperature=0)\nchain = model | StrOutputParser() | json.loads\nchain.invoke(challenge)\nfinal_chain = simple_chain.with_fallbacks([chain])\nfinal_chain.invoke(challenge)\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "model = ChatOpenAI(temperature=0)\nchain = model | StrOutputParser() | json.loads\nchain.invoke(challenge)\nfinal_chain = simple_chain.with_fallbacks([chain])\nfinal_chain.invoke(challenge)\n#----------------------------------------------------------------\n#----------------------------------------------------------------\nprint(\"\\n\\n Interface with SYNC & ASYNC call : \\n\\n\")\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "chain = model | StrOutputParser() | json.loads\nchain.invoke(challenge)\nfinal_chain = simple_chain.with_fallbacks([chain])\nfinal_chain.invoke(challenge)\n#----------------------------------------------------------------\n#----------------------------------------------------------------\nprint(\"\\n\\n Interface with SYNC & ASYNC call : \\n\\n\")\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "final_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "final_chain = simple_chain.with_fallbacks([chain])\nfinal_chain.invoke(challenge)\n#----------------------------------------------------------------\n#----------------------------------------------------------------\nprint(\"\\n\\n Interface with SYNC & ASYNC call : \\n\\n\")\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\nresp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "model = ChatOpenAI()\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\nresp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})\n    print(\"\\nResponse from async call : \", resp_invoke_async)\nimport asyncio\nasyncio.run(async_call())",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "output_parser",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "output_parser = StrOutputParser()\nchain = prompt | model | output_parser\nresp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})\n    print(\"\\nResponse from async call : \", resp_invoke_async)\nimport asyncio\nasyncio.run(async_call())\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "chain = prompt | model | output_parser\nresp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})\n    print(\"\\nResponse from async call : \", resp_invoke_async)\nimport asyncio\nasyncio.run(async_call())\n#----------------------------------------------------------------\nresp_chain_batch_sync = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "resp_invoke_sync",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "resp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})\n    print(\"\\nResponse from async call : \", resp_invoke_async)\nimport asyncio\nasyncio.run(async_call())\n#----------------------------------------------------------------\nresp_chain_batch_sync = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])\nprint(\"\\n\\nResponse from sync batch call : \\n\", resp_chain_batch_sync)",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "resp_chain_batch_sync",
        "kind": 5,
        "importPath": "Functions_Langchain.L2-lcel-student",
        "description": "Functions_Langchain.L2-lcel-student",
        "peekOfCode": "resp_chain_batch_sync = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])\nprint(\"\\n\\nResponse from sync batch call : \\n\", resp_chain_batch_sync)\nfor t in chain.stream({\"topic\": \"bears\"}):\n    print(t)",
        "detail": "Functions_Langchain.L2-lcel-student",
        "documentation": {}
    },
    {
        "label": "User",
        "kind": 6,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "class User():\n    def __init__(self, name: str, age: int, email: str):\n        self.name = name\n        self.age = age\n        self.email = email\nfoo = User(name='Joe', age=32, email='joe@gmail.com')\nprint(foo.name)\nhoo = User(name='Hoe', age='mid', email='joe@gmail.com')\nprint(hoo.age)\nclass pUser(BaseModel):",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "pUser",
        "kind": 6,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "class pUser(BaseModel):\n    name: str\n    age: int\n    email: str\npoo = pUser(name='Poo', age=32, email='joe@gmail.com')\nprint(poo.name)\n# moo=pUser(name='Moo', age='old', email='joe@another.com')\n# print(moo.age)\nclass Class(BaseModel):\n    students: list[pUser]",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "Class",
        "kind": 6,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "class Class(BaseModel):\n    students: list[pUser]\nMoo = Class(students=[pUser(name='Moo', age=55, email='joe@another.com')])\nprint(Moo)\n#----------------------------------------------------------------\n# Pydantic to OpenAI function definition\nclass WeatherSearch(BaseModel):\n    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n    airport_code: str = Field(description=\"airport code to get the weather for\")\nfrom langchain.utils.openai_functions import convert_pydantic_to_openai_function",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "WeatherSearch",
        "kind": 6,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "class WeatherSearch(BaseModel):\n    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n    airport_code: str = Field(description=\"airport code to get the weather for\")\nfrom langchain.utils.openai_functions import convert_pydantic_to_openai_function\nweather_function = convert_pydantic_to_openai_function(WeatherSearch)\nprint (weather_function)\nclass WeatherSearch1(BaseModel):\n    airport_code: str = Field(description=\"airport code to get weather for\")\nweather_function_1 = convert_pydantic_to_openai_function(WeatherSearch1)\nprint (weather_function_1)",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "WeatherSearch1",
        "kind": 6,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "class WeatherSearch1(BaseModel):\n    airport_code: str = Field(description=\"airport code to get weather for\")\nweather_function_1 = convert_pydantic_to_openai_function(WeatherSearch1)\nprint (weather_function_1)\nclass WeatherSearch2(BaseModel):\n    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n    airport_code: str\nweather_function_2 = convert_pydantic_to_openai_function(WeatherSearch2)\nprint (weather_function_2)\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "WeatherSearch2",
        "kind": 6,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "class WeatherSearch2(BaseModel):\n    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n    airport_code: str\nweather_function_2 = convert_pydantic_to_openai_function(WeatherSearch2)\nprint (weather_function_2)\n#----------------------------------------------------------------\nfrom langchain_openai import ChatOpenAI\n# from Functions_Langchain.L1-openai_functions_student import functions\nmodel = ChatOpenAI()\nresp_wtf = model.invoke(\"What is the weather in SF today?\", functions=[weather_function])",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "ArtistSearch",
        "kind": 6,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "class ArtistSearch(BaseModel):\n    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n    artist_name: str = Field(description=\"name of artist to look up\")\n    n: int = Field(description=\"number of results\")\nfunctions = [\n    convert_pydantic_to_openai_function(ArtistSearch),\n    convert_pydantic_to_openai_function(WeatherSearch)\n]\nmodel_with_mult_functions = model.bind(functions=functions)\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"what is the weather in sf?\")",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "foo",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "foo = User(name='Joe', age=32, email='joe@gmail.com')\nprint(foo.name)\nhoo = User(name='Hoe', age='mid', email='joe@gmail.com')\nprint(hoo.age)\nclass pUser(BaseModel):\n    name: str\n    age: int\n    email: str\npoo = pUser(name='Poo', age=32, email='joe@gmail.com')\nprint(poo.name)",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "hoo",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "hoo = User(name='Hoe', age='mid', email='joe@gmail.com')\nprint(hoo.age)\nclass pUser(BaseModel):\n    name: str\n    age: int\n    email: str\npoo = pUser(name='Poo', age=32, email='joe@gmail.com')\nprint(poo.name)\n# moo=pUser(name='Moo', age='old', email='joe@another.com')\n# print(moo.age)",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "poo",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "poo = pUser(name='Poo', age=32, email='joe@gmail.com')\nprint(poo.name)\n# moo=pUser(name='Moo', age='old', email='joe@another.com')\n# print(moo.age)\nclass Class(BaseModel):\n    students: list[pUser]\nMoo = Class(students=[pUser(name='Moo', age=55, email='joe@another.com')])\nprint(Moo)\n#----------------------------------------------------------------\n# Pydantic to OpenAI function definition",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "Moo",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "Moo = Class(students=[pUser(name='Moo', age=55, email='joe@another.com')])\nprint(Moo)\n#----------------------------------------------------------------\n# Pydantic to OpenAI function definition\nclass WeatherSearch(BaseModel):\n    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n    airport_code: str = Field(description=\"airport code to get the weather for\")\nfrom langchain.utils.openai_functions import convert_pydantic_to_openai_function\nweather_function = convert_pydantic_to_openai_function(WeatherSearch)\nprint (weather_function)",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "weather_function",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "weather_function = convert_pydantic_to_openai_function(WeatherSearch)\nprint (weather_function)\nclass WeatherSearch1(BaseModel):\n    airport_code: str = Field(description=\"airport code to get weather for\")\nweather_function_1 = convert_pydantic_to_openai_function(WeatherSearch1)\nprint (weather_function_1)\nclass WeatherSearch2(BaseModel):\n    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n    airport_code: str\nweather_function_2 = convert_pydantic_to_openai_function(WeatherSearch2)",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "weather_function_1",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "weather_function_1 = convert_pydantic_to_openai_function(WeatherSearch1)\nprint (weather_function_1)\nclass WeatherSearch2(BaseModel):\n    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n    airport_code: str\nweather_function_2 = convert_pydantic_to_openai_function(WeatherSearch2)\nprint (weather_function_2)\n#----------------------------------------------------------------\nfrom langchain_openai import ChatOpenAI\n# from Functions_Langchain.L1-openai_functions_student import functions",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "weather_function_2",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "weather_function_2 = convert_pydantic_to_openai_function(WeatherSearch2)\nprint (weather_function_2)\n#----------------------------------------------------------------\nfrom langchain_openai import ChatOpenAI\n# from Functions_Langchain.L1-openai_functions_student import functions\nmodel = ChatOpenAI()\nresp_wtf = model.invoke(\"What is the weather in SF today?\", functions=[weather_function])\nprint(resp_wtf)\nmodel_with_function = model.bind(functions=[weather_function])\nresp_wtf_bind = model_with_function.invoke(\"what is the weather in sf?\")",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "model = ChatOpenAI()\nresp_wtf = model.invoke(\"What is the weather in SF today?\", functions=[weather_function])\nprint(resp_wtf)\nmodel_with_function = model.bind(functions=[weather_function])\nresp_wtf_bind = model_with_function.invoke(\"what is the weather in sf?\")\nprint(resp_wtf_bind)\n#----------------------------------------------------------------\n# Forcing it to use a function\nmodel_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})\nresp_wtf_bind_fc = model_with_forced_function.invoke(\"what is the weather in sf?\")",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "resp_wtf",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "resp_wtf = model.invoke(\"What is the weather in SF today?\", functions=[weather_function])\nprint(resp_wtf)\nmodel_with_function = model.bind(functions=[weather_function])\nresp_wtf_bind = model_with_function.invoke(\"what is the weather in sf?\")\nprint(resp_wtf_bind)\n#----------------------------------------------------------------\n# Forcing it to use a function\nmodel_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})\nresp_wtf_bind_fc = model_with_forced_function.invoke(\"what is the weather in sf?\")\nprint(f\"\\n\\n Forcing it to use a function \\n {resp_wtf_bind_fc}\")",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "model_with_function",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "model_with_function = model.bind(functions=[weather_function])\nresp_wtf_bind = model_with_function.invoke(\"what is the weather in sf?\")\nprint(resp_wtf_bind)\n#----------------------------------------------------------------\n# Forcing it to use a function\nmodel_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})\nresp_wtf_bind_fc = model_with_forced_function.invoke(\"what is the weather in sf?\")\nprint(f\"\\n\\n Forcing it to use a function \\n {resp_wtf_bind_fc}\")\n#----------------------------------------------------------------\n# Using in a chain",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "resp_wtf_bind",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "resp_wtf_bind = model_with_function.invoke(\"what is the weather in sf?\")\nprint(resp_wtf_bind)\n#----------------------------------------------------------------\n# Forcing it to use a function\nmodel_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})\nresp_wtf_bind_fc = model_with_forced_function.invoke(\"what is the weather in sf?\")\nprint(f\"\\n\\n Forcing it to use a function \\n {resp_wtf_bind_fc}\")\n#----------------------------------------------------------------\n# Using in a chain\nfrom langchain.prompts import ChatPromptTemplate",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "model_with_forced_function",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "model_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})\nresp_wtf_bind_fc = model_with_forced_function.invoke(\"what is the weather in sf?\")\nprint(f\"\\n\\n Forcing it to use a function \\n {resp_wtf_bind_fc}\")\n#----------------------------------------------------------------\n# Using in a chain\nfrom langchain.prompts import ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages([\n    ('system', \"You are a helpful assistant\"),\n    (\"user\", \"{input}\")    \n])",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "resp_wtf_bind_fc",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "resp_wtf_bind_fc = model_with_forced_function.invoke(\"what is the weather in sf?\")\nprint(f\"\\n\\n Forcing it to use a function \\n {resp_wtf_bind_fc}\")\n#----------------------------------------------------------------\n# Using in a chain\nfrom langchain.prompts import ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages([\n    ('system', \"You are a helpful assistant\"),\n    (\"user\", \"{input}\")    \n])\nchain = prompt | model_with_function",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    ('system', \"You are a helpful assistant\"),\n    (\"user\", \"{input}\")    \n])\nchain = prompt | model_with_function\nresp_model_wf_chain = chain.invoke({\"input\": \"what is the weather in sf?\"})\nprint(f\"\\n\\n Using in a chain \\n {resp_model_wf_chain}\")\n#----------------------------------------------------------------\n# Using multiple functions\nclass ArtistSearch(BaseModel):",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "chain = prompt | model_with_function\nresp_model_wf_chain = chain.invoke({\"input\": \"what is the weather in sf?\"})\nprint(f\"\\n\\n Using in a chain \\n {resp_model_wf_chain}\")\n#----------------------------------------------------------------\n# Using multiple functions\nclass ArtistSearch(BaseModel):\n    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n    artist_name: str = Field(description=\"name of artist to look up\")\n    n: int = Field(description=\"number of results\")\nfunctions = [",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "resp_model_wf_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "resp_model_wf_chain = chain.invoke({\"input\": \"what is the weather in sf?\"})\nprint(f\"\\n\\n Using in a chain \\n {resp_model_wf_chain}\")\n#----------------------------------------------------------------\n# Using multiple functions\nclass ArtistSearch(BaseModel):\n    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n    artist_name: str = Field(description=\"name of artist to look up\")\n    n: int = Field(description=\"number of results\")\nfunctions = [\n    convert_pydantic_to_openai_function(ArtistSearch),",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "functions = [\n    convert_pydantic_to_openai_function(ArtistSearch),\n    convert_pydantic_to_openai_function(WeatherSearch)\n]\nmodel_with_mult_functions = model.bind(functions=functions)\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"what is the weather in sf?\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"what are three songs by taylor swift?\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"hi!\")",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "model_with_mult_functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "model_with_mult_functions = model.bind(functions=functions)\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"what is the weather in sf?\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"what are three songs by taylor swift?\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"hi!\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "resp_model_wmf_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "resp_model_wmf_chain = model_with_mult_functions.invoke(\"what is the weather in sf?\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"what are three songs by taylor swift?\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"hi!\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "resp_model_wmf_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "resp_model_wmf_chain = model_with_mult_functions.invoke(\"what are three songs by taylor swift?\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\nresp_model_wmf_chain = model_with_mult_functions.invoke(\"hi!\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "resp_model_wmf_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L3-function-calling-student",
        "description": "Functions_Langchain.L3-function-calling-student",
        "peekOfCode": "resp_model_wmf_chain = model_with_mult_functions.invoke(\"hi!\")\nprint(f\"\\n\\n Using multiple functions \\n {resp_model_wmf_chain}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L3-function-calling-student",
        "documentation": {}
    },
    {
        "label": "Tagging",
        "kind": 6,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "class Tagging(BaseModel):\n    \"\"\"Tag the piece of text with particular info.\"\"\"\n    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")\ntagging_openai = convert_pydantic_to_openai_function(Tagging)\nprint(f\"\\n Tagging function compatible with OpenAI: \\n{tagging_openai}\")\n#----------------------------------------------------------------\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n# from Functions_Langchain.L1-openai_functions_student import functions",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "Person",
        "kind": 6,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "class Person(BaseModel):\n    \"\"\"Information about a person.\"\"\"\n    name: str = Field(description=\"person's name\")\n    age: Optional[int] = Field(description=\"person's age\")\nclass Information(BaseModel):\n    \"\"\"Information to extract.\"\"\"\n    people: List[Person] = Field(description=\"List of info about people\")\nconvert_pydantic_to_openai_function(Information)\nextraction_functions = [convert_pydantic_to_openai_function(Information)]\nextraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "Information",
        "kind": 6,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "class Information(BaseModel):\n    \"\"\"Information to extract.\"\"\"\n    people: List[Person] = Field(description=\"List of info about people\")\nconvert_pydantic_to_openai_function(Information)\nextraction_functions = [convert_pydantic_to_openai_function(Information)]\nextraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})\nresp_fn_chain_4 = extraction_model.invoke(\"Joe is 30, his mom is Martha\")\nprint(f\"\\n\\nExtraction with Function Calling - 1: \\n{resp_fn_chain_4}\")\n#----------------------------------------------------------------\nprompt = ChatPromptTemplate.from_messages([",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "Overview",
        "kind": 6,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "class Overview(BaseModel):\n    \"\"\"Overview of a section of a text.\"\"\"\n    summary: str = Field(description=\"Provide a concise summary of the content.\")\n    language: str = Field(description=\"Provide the language that the content is written in.\")\n    keywords: str = Field(description=\"Provide keywords related to the content.\")\noverview_tagging_function = [\n    convert_pydantic_to_openai_function(Overview)\n]\ntagging_model = model.bind(\n    functions=overview_tagging_function,",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "Paper",
        "kind": 6,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "class Paper(BaseModel):\n    \"\"\"Information about papers mentioned.\"\"\"\n    title: str\n    author: Optional[str]\nclass Info(BaseModel):\n    \"\"\"Information to extract\"\"\"\n    papers: List[Paper]\npaper_extraction_function = [\n    convert_pydantic_to_openai_function(Info)\n]",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "Info",
        "kind": 6,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "class Info(BaseModel):\n    \"\"\"Information to extract\"\"\"\n    papers: List[Paper]\npaper_extraction_function = [\n    convert_pydantic_to_openai_function(Info)\n]\nprint(f\"\\n\\npaper_extraction_function content : \\n{paper_extraction_function}\")\nextraction_model = model.bind(\n    functions=paper_extraction_function, \n    function_call={\"name\":\"Info\"}",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "flatten",
        "kind": 2,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "def flatten(matrix):\n    flat_list = []\n    for row in matrix:\n        flat_list += row\n    return flat_list\nflatten([[1,2], [3,4]])\nprint(f\"\\n\\nContent of first Split : \\n{splits[0]}\")\n#----------------------------------------------------------------\nfrom langchain.schema.runnable import RunnableLambda\nprep = RunnableLambda(",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "tagging_openai",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "tagging_openai = convert_pydantic_to_openai_function(Tagging)\nprint(f\"\\n Tagging function compatible with OpenAI: \\n{tagging_openai}\")\n#----------------------------------------------------------------\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n# from Functions_Langchain.L1-openai_functions_student import functions\nmodel = ChatOpenAI(temperature=0)\ntagging_functions = [convert_pydantic_to_openai_function(Tagging)]\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Think carefully, and then tag the text as instructed\"),",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "model = ChatOpenAI(temperature=0)\ntagging_functions = [convert_pydantic_to_openai_function(Tagging)]\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n    (\"user\", \"{input}\")\n])\nmodel_with_functions = model.bind(\n    functions=tagging_functions,\n    function_call={\"name\": \"Tagging\"}\n)",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "tagging_functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n    (\"user\", \"{input}\")\n])\nmodel_with_functions = model.bind(\n    functions=tagging_functions,\n    function_call={\"name\": \"Tagging\"}\n)\ntagging_chain = prompt | model_with_functions",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n    (\"user\", \"{input}\")\n])\nmodel_with_functions = model.bind(\n    functions=tagging_functions,\n    function_call={\"name\": \"Tagging\"}\n)\ntagging_chain = prompt | model_with_functions\nresp_fn_chain_1 = tagging_chain.invoke({\"input\": \"I love Langchain.\"})",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "model_with_functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "model_with_functions = model.bind(\n    functions=tagging_functions,\n    function_call={\"name\": \"Tagging\"}\n)\ntagging_chain = prompt | model_with_functions\nresp_fn_chain_1 = tagging_chain.invoke({\"input\": \"I love Langchain.\"})\nprint(f\"\\n\\nTagging with Function Calling for English: \\n{resp_fn_chain_1}\")\n# resp_fn_chain_2 = tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})\nresp_fn_chain_2 = tagging_chain.invoke({\"input\": \"amar aaj kichu bhalo lagche na\"})\nprint(f\"\\n\\nTagging with Function Calling for Bengali : \\n{resp_fn_chain_2}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "tagging_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "tagging_chain = prompt | model_with_functions\nresp_fn_chain_1 = tagging_chain.invoke({\"input\": \"I love Langchain.\"})\nprint(f\"\\n\\nTagging with Function Calling for English: \\n{resp_fn_chain_1}\")\n# resp_fn_chain_2 = tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})\nresp_fn_chain_2 = tagging_chain.invoke({\"input\": \"amar aaj kichu bhalo lagche na\"})\nprint(f\"\\n\\nTagging with Function Calling for Bengali : \\n{resp_fn_chain_2}\")\n#----------------------------------------------------------------\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\ntagging_chain_json = prompt | model_with_functions | JsonOutputFunctionsParser()\nresp_fn_chain_3 = tagging_chain.invoke({\"input\": \"kaha ho tum\"})",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_1",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_1 = tagging_chain.invoke({\"input\": \"I love Langchain.\"})\nprint(f\"\\n\\nTagging with Function Calling for English: \\n{resp_fn_chain_1}\")\n# resp_fn_chain_2 = tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})\nresp_fn_chain_2 = tagging_chain.invoke({\"input\": \"amar aaj kichu bhalo lagche na\"})\nprint(f\"\\n\\nTagging with Function Calling for Bengali : \\n{resp_fn_chain_2}\")\n#----------------------------------------------------------------\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\ntagging_chain_json = prompt | model_with_functions | JsonOutputFunctionsParser()\nresp_fn_chain_3 = tagging_chain.invoke({\"input\": \"kaha ho tum\"})\nprint(f\"\\n\\nTagging with Function Calling for Hindi: \\n{resp_fn_chain_3}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_2",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_2 = tagging_chain.invoke({\"input\": \"amar aaj kichu bhalo lagche na\"})\nprint(f\"\\n\\nTagging with Function Calling for Bengali : \\n{resp_fn_chain_2}\")\n#----------------------------------------------------------------\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\ntagging_chain_json = prompt | model_with_functions | JsonOutputFunctionsParser()\nresp_fn_chain_3 = tagging_chain.invoke({\"input\": \"kaha ho tum\"})\nprint(f\"\\n\\nTagging with Function Calling for Hindi: \\n{resp_fn_chain_3}\")\n#----------------------------------------------------------------\n# Extraction\nfrom typing import Optional",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "tagging_chain_json",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "tagging_chain_json = prompt | model_with_functions | JsonOutputFunctionsParser()\nresp_fn_chain_3 = tagging_chain.invoke({\"input\": \"kaha ho tum\"})\nprint(f\"\\n\\nTagging with Function Calling for Hindi: \\n{resp_fn_chain_3}\")\n#----------------------------------------------------------------\n# Extraction\nfrom typing import Optional\nclass Person(BaseModel):\n    \"\"\"Information about a person.\"\"\"\n    name: str = Field(description=\"person's name\")\n    age: Optional[int] = Field(description=\"person's age\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_3",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_3 = tagging_chain.invoke({\"input\": \"kaha ho tum\"})\nprint(f\"\\n\\nTagging with Function Calling for Hindi: \\n{resp_fn_chain_3}\")\n#----------------------------------------------------------------\n# Extraction\nfrom typing import Optional\nclass Person(BaseModel):\n    \"\"\"Information about a person.\"\"\"\n    name: str = Field(description=\"person's name\")\n    age: Optional[int] = Field(description=\"person's age\")\nclass Information(BaseModel):",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "extraction_functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "extraction_functions = [convert_pydantic_to_openai_function(Information)]\nextraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})\nresp_fn_chain_4 = extraction_model.invoke(\"Joe is 30, his mom is Martha\")\nprint(f\"\\n\\nExtraction with Function Calling - 1: \\n{resp_fn_chain_4}\")\n#----------------------------------------------------------------\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n    (\"human\", \"{input}\")\n])\nextraction_chain = prompt | extraction_model",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "extraction_model",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})\nresp_fn_chain_4 = extraction_model.invoke(\"Joe is 30, his mom is Martha\")\nprint(f\"\\n\\nExtraction with Function Calling - 1: \\n{resp_fn_chain_4}\")\n#----------------------------------------------------------------\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n    (\"human\", \"{input}\")\n])\nextraction_chain = prompt | extraction_model\nresp_fn_chain_5 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_4",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_4 = extraction_model.invoke(\"Joe is 30, his mom is Martha\")\nprint(f\"\\n\\nExtraction with Function Calling - 1: \\n{resp_fn_chain_4}\")\n#----------------------------------------------------------------\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n    (\"human\", \"{input}\")\n])\nextraction_chain = prompt | extraction_model\nresp_fn_chain_5 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain - 1: \\n{resp_fn_chain_5}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n    (\"human\", \"{input}\")\n])\nextraction_chain = prompt | extraction_model\nresp_fn_chain_5 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain - 1: \\n{resp_fn_chain_5}\")\nextraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()\nresp_fn_chain_6 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain +json - 1: \\n{resp_fn_chain_6}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "extraction_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "extraction_chain = prompt | extraction_model\nresp_fn_chain_5 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain - 1: \\n{resp_fn_chain_5}\")\nextraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()\nresp_fn_chain_6 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain +json - 1: \\n{resp_fn_chain_6}\")\nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\nextraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")\nresp_fn_chain_7 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain + json + key - 1: \\n{resp_fn_chain_7}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_5",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_5 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain - 1: \\n{resp_fn_chain_5}\")\nextraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()\nresp_fn_chain_6 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain +json - 1: \\n{resp_fn_chain_6}\")\nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\nextraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")\nresp_fn_chain_7 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain + json + key - 1: \\n{resp_fn_chain_7}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "extraction_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()\nresp_fn_chain_6 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain +json - 1: \\n{resp_fn_chain_6}\")\nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\nextraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")\nresp_fn_chain_7 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain + json + key - 1: \\n{resp_fn_chain_7}\")\n#----------------------------------------------------------------\n# Doing it for real\nfrom langchain.document_loaders import WebBaseLoader",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_6",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_6 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain +json - 1: \\n{resp_fn_chain_6}\")\nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\nextraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")\nresp_fn_chain_7 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain + json + key - 1: \\n{resp_fn_chain_7}\")\n#----------------------------------------------------------------\n# Doing it for real\nfrom langchain.document_loaders import WebBaseLoader\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "extraction_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")\nresp_fn_chain_7 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain + json + key - 1: \\n{resp_fn_chain_7}\")\n#----------------------------------------------------------------\n# Doing it for real\nfrom langchain.document_loaders import WebBaseLoader\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\ndocuments = loader.load()\ndoc = documents[0]\npage_content = doc.page_content[:10000]",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_7",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_7 = extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nprint(f\"\\n\\nExtraction with Function Calling + chain + json + key - 1: \\n{resp_fn_chain_7}\")\n#----------------------------------------------------------------\n# Doing it for real\nfrom langchain.document_loaders import WebBaseLoader\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\ndocuments = loader.load()\ndoc = documents[0]\npage_content = doc.page_content[:10000]\nprint(f\"\\n\\n Pages from web content \\n{page_content[:1000]}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\ndocuments = loader.load()\ndoc = documents[0]\npage_content = doc.page_content[:10000]\nprint(f\"\\n\\n Pages from web content \\n{page_content[:1000]}\")\nclass Overview(BaseModel):\n    \"\"\"Overview of a section of a text.\"\"\"\n    summary: str = Field(description=\"Provide a concise summary of the content.\")\n    language: str = Field(description=\"Provide the language that the content is written in.\")\n    keywords: str = Field(description=\"Provide keywords related to the content.\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "documents = loader.load()\ndoc = documents[0]\npage_content = doc.page_content[:10000]\nprint(f\"\\n\\n Pages from web content \\n{page_content[:1000]}\")\nclass Overview(BaseModel):\n    \"\"\"Overview of a section of a text.\"\"\"\n    summary: str = Field(description=\"Provide a concise summary of the content.\")\n    language: str = Field(description=\"Provide the language that the content is written in.\")\n    keywords: str = Field(description=\"Provide keywords related to the content.\")\noverview_tagging_function = [",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "doc = documents[0]\npage_content = doc.page_content[:10000]\nprint(f\"\\n\\n Pages from web content \\n{page_content[:1000]}\")\nclass Overview(BaseModel):\n    \"\"\"Overview of a section of a text.\"\"\"\n    summary: str = Field(description=\"Provide a concise summary of the content.\")\n    language: str = Field(description=\"Provide the language that the content is written in.\")\n    keywords: str = Field(description=\"Provide keywords related to the content.\")\noverview_tagging_function = [\n    convert_pydantic_to_openai_function(Overview)",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "page_content",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "page_content = doc.page_content[:10000]\nprint(f\"\\n\\n Pages from web content \\n{page_content[:1000]}\")\nclass Overview(BaseModel):\n    \"\"\"Overview of a section of a text.\"\"\"\n    summary: str = Field(description=\"Provide a concise summary of the content.\")\n    language: str = Field(description=\"Provide the language that the content is written in.\")\n    keywords: str = Field(description=\"Provide keywords related to the content.\")\noverview_tagging_function = [\n    convert_pydantic_to_openai_function(Overview)\n]",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "overview_tagging_function",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "overview_tagging_function = [\n    convert_pydantic_to_openai_function(Overview)\n]\ntagging_model = model.bind(\n    functions=overview_tagging_function,\n    function_call={\"name\":\"Overview\"}\n)\ntagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()\nresp_fn_chain_page_1 = tagging_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + json - 1: \\n{resp_fn_chain_page_1}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "tagging_model",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "tagging_model = model.bind(\n    functions=overview_tagging_function,\n    function_call={\"name\":\"Overview\"}\n)\ntagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()\nresp_fn_chain_page_1 = tagging_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + json - 1: \\n{resp_fn_chain_page_1}\")\n#----------------------------------------------------------------\nclass Paper(BaseModel):\n    \"\"\"Information about papers mentioned.\"\"\"",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "tagging_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()\nresp_fn_chain_page_1 = tagging_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + json - 1: \\n{resp_fn_chain_page_1}\")\n#----------------------------------------------------------------\nclass Paper(BaseModel):\n    \"\"\"Information about papers mentioned.\"\"\"\n    title: str\n    author: Optional[str]\nclass Info(BaseModel):\n    \"\"\"Information to extract\"\"\"",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_page_1",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_page_1 = tagging_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + json - 1: \\n{resp_fn_chain_page_1}\")\n#----------------------------------------------------------------\nclass Paper(BaseModel):\n    \"\"\"Information about papers mentioned.\"\"\"\n    title: str\n    author: Optional[str]\nclass Info(BaseModel):\n    \"\"\"Information to extract\"\"\"\n    papers: List[Paper]",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "paper_extraction_function",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "paper_extraction_function = [\n    convert_pydantic_to_openai_function(Info)\n]\nprint(f\"\\n\\npaper_extraction_function content : \\n{paper_extraction_function}\")\nextraction_model = model.bind(\n    functions=paper_extraction_function, \n    function_call={\"name\":\"Info\"}\n)\nextraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\nresp_fn_chain_page_2 = extraction_chain.invoke({\"input\": page_content})",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "extraction_model",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "extraction_model = model.bind(\n    functions=paper_extraction_function, \n    function_call={\"name\":\"Info\"}\n)\nextraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\nresp_fn_chain_page_2 = extraction_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + Key + json - 1: \\n{resp_fn_chain_page_2}\")\n#----------------------------------------------------------------\ntemplate = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article. \nDo not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! ",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "extraction_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\nresp_fn_chain_page_2 = extraction_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + Key + json - 1: \\n{resp_fn_chain_page_2}\")\n#----------------------------------------------------------------\ntemplate = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article. \nDo not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! \nJust return an empty list. Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", \"{input}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_page_2",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_page_2 = extraction_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + Key + json - 1: \\n{resp_fn_chain_page_2}\")\n#----------------------------------------------------------------\ntemplate = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article. \nDo not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! \nJust return an empty list. Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", \"{input}\")\n])",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "template = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article. \nDo not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! \nJust return an empty list. Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", \"{input}\")\n])\nextraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\nresp_fn_chain_page_3 = extraction_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + Key + json - 2: \\n{resp_fn_chain_page_2}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", \"{input}\")\n])\nextraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\nresp_fn_chain_page_3 = extraction_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + Key + json - 2: \\n{resp_fn_chain_page_2}\")\nresp_fn_chain_page_3 = extraction_chain.invoke({\"input\": \"hi\"})\nprint(f\"\\n\\n Extraction of Hi with Function Calling + chain + Key + json - 2: \\n{resp_fn_chain_page_3}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "extraction_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\nresp_fn_chain_page_3 = extraction_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + Key + json - 2: \\n{resp_fn_chain_page_2}\")\nresp_fn_chain_page_3 = extraction_chain.invoke({\"input\": \"hi\"})\nprint(f\"\\n\\n Extraction of Hi with Function Calling + chain + Key + json - 2: \\n{resp_fn_chain_page_3}\")\n#----------------------------------------------------------------\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\nsplits = text_splitter.split_text(doc.page_content)\nlength = len(splits)",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_page_3",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_page_3 = extraction_chain.invoke({\"input\": page_content})\nprint(f\"\\n\\n Extraction of Web-Page Summary with Function Calling + chain + Key + json - 2: \\n{resp_fn_chain_page_2}\")\nresp_fn_chain_page_3 = extraction_chain.invoke({\"input\": \"hi\"})\nprint(f\"\\n\\n Extraction of Hi with Function Calling + chain + Key + json - 2: \\n{resp_fn_chain_page_3}\")\n#----------------------------------------------------------------\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\nsplits = text_splitter.split_text(doc.page_content)\nlength = len(splits)\nprint(f\"\\n\\n Length of RecursiveCharacterTextSplitter : {length}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_page_3",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_page_3 = extraction_chain.invoke({\"input\": \"hi\"})\nprint(f\"\\n\\n Extraction of Hi with Function Calling + chain + Key + json - 2: \\n{resp_fn_chain_page_3}\")\n#----------------------------------------------------------------\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\nsplits = text_splitter.split_text(doc.page_content)\nlength = len(splits)\nprint(f\"\\n\\n Length of RecursiveCharacterTextSplitter : {length}\")\ndef flatten(matrix):\n    flat_list = []",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\nsplits = text_splitter.split_text(doc.page_content)\nlength = len(splits)\nprint(f\"\\n\\n Length of RecursiveCharacterTextSplitter : {length}\")\ndef flatten(matrix):\n    flat_list = []\n    for row in matrix:\n        flat_list += row\n    return flat_list\nflatten([[1,2], [3,4]])",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "splits",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "splits = text_splitter.split_text(doc.page_content)\nlength = len(splits)\nprint(f\"\\n\\n Length of RecursiveCharacterTextSplitter : {length}\")\ndef flatten(matrix):\n    flat_list = []\n    for row in matrix:\n        flat_list += row\n    return flat_list\nflatten([[1,2], [3,4]])\nprint(f\"\\n\\nContent of first Split : \\n{splits[0]}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "length",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "length = len(splits)\nprint(f\"\\n\\n Length of RecursiveCharacterTextSplitter : {length}\")\ndef flatten(matrix):\n    flat_list = []\n    for row in matrix:\n        flat_list += row\n    return flat_list\nflatten([[1,2], [3,4]])\nprint(f\"\\n\\nContent of first Split : \\n{splits[0]}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "prep",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "prep = RunnableLambda(\n    lambda x: [{\"input\": docs} for docs in text_splitter.split_text(x)]\n)\n# prep.invoke(\"hi\")\nchain = prep | extraction_chain.map() | flatten\nresp_fn_chain_page_runnable_1 = chain.invoke(doc.page_content)\nprint(f\"\\n\\n Extraction of Hi with Function Calling + chain + flatten - 2: \\n{resp_fn_chain_page_runnable_1}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "chain = prep | extraction_chain.map() | flatten\nresp_fn_chain_page_runnable_1 = chain.invoke(doc.page_content)\nprint(f\"\\n\\n Extraction of Hi with Function Calling + chain + flatten - 2: \\n{resp_fn_chain_page_runnable_1}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "resp_fn_chain_page_runnable_1",
        "kind": 5,
        "importPath": "Functions_Langchain.L4-tagging-and-extraction-student",
        "description": "Functions_Langchain.L4-tagging-and-extraction-student",
        "peekOfCode": "resp_fn_chain_page_runnable_1 = chain.invoke(doc.page_content)\nprint(f\"\\n\\n Extraction of Hi with Function Calling + chain + flatten - 2: \\n{resp_fn_chain_page_runnable_1}\")",
        "detail": "Functions_Langchain.L4-tagging-and-extraction-student",
        "documentation": {}
    },
    {
        "label": "SearchInput",
        "kind": 6,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "class SearchInput(BaseModel):\n    query: str = Field(description=\"Thing to search for\")\n@tool(args_schema=SearchInput)\ndef search(query: str) -> str:\n    \"\"\"Search for weather online\"\"\"\n    return \"42f\"\nprint(f\"search.name : {search.name}\")\nprint(f\"search.description : {search.description}\")\nprint(f\"search.args : {search.args}\")\nprint(f\"search.run : {search.run(\"sf\")}\")",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "OpenMeteoInput",
        "kind": 6,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "class OpenMeteoInput(BaseModel):\n    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n@tool(args_schema=OpenMeteoInput)\ndef get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {\n        'latitude': latitude,",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 2,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "def search(query: str) -> str:\n    \"\"\"Search for weather online\"\"\"\n    return \"42f\"\nprint(f\"search.name : {search.name}\")\nprint(f\"search.description : {search.description}\")\nprint(f\"search.args : {search.args}\")\n#----------------------------------------------------------------\nfrom pydantic import BaseModel, Field\nclass SearchInput(BaseModel):\n    query: str = Field(description=\"Thing to search for\")",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 2,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "def search(query: str) -> str:\n    \"\"\"Search for weather online\"\"\"\n    return \"42f\"\nprint(f\"search.name : {search.name}\")\nprint(f\"search.description : {search.description}\")\nprint(f\"search.args : {search.args}\")\nprint(f\"search.run : {search.run(\"sf\")}\")\n#----------------------------------------------------------------\nimport requests\nfrom pydantic import BaseModel, Field",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "get_current_temperature",
        "kind": 2,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "def get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {\n        'latitude': latitude,\n        'longitude': longitude,\n        'hourly': 'temperature_2m',\n        'forecast_days': 1,\n    }",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "search_wikipedia",
        "kind": 2,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "def search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n    for page_title in page_titles[: 3]:\n        try:\n            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n        except (\n            self.wiki_client.exceptions.PageError,",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "route",
        "kind": 2,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "def route(result):\n    if isinstance(result, AgentFinish):\n        return result.return_values['output']\n    else:\n        tools = {\n            \"search_wikipedia\": search_wikipedia, \n            \"get_current_temperature\": get_current_temperature,\n        }\n        return tools[result.tool].run(result.tool_input)\nchain = prompt | model | OpenAIFunctionsAgentOutputParser() | route",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "get_current_temperature_openai",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "get_current_temperature_openai = format_tool_to_openai_function(get_current_temperature)\nprint(f\"\\n\\nget_current_temperature in Openai Format \\n : {get_current_temperature_openai}\")\nget_current_temperature_openai_2 = get_current_temperature({\"latitude\": 13, \"longitude\": 14})\nprint(f\"\\n\\nget_current_temperature is \\n : {get_current_temperature_openai_2}\")\n#----------------------------------------------------------------\nimport wikipedia\n@tool\ndef search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "get_current_temperature_openai_2",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "get_current_temperature_openai_2 = get_current_temperature({\"latitude\": 13, \"longitude\": 14})\nprint(f\"\\n\\nget_current_temperature is \\n : {get_current_temperature_openai_2}\")\n#----------------------------------------------------------------\nimport wikipedia\n@tool\ndef search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n    for page_title in page_titles[: 3]:",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "res_wiki_search",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "res_wiki_search = search_wikipedia({\"query\": \"langchain\"})\nprint(f\"\\n\\nWikipedia Search Result : \\n : {res_wiki_search}\")\n#----------------------------------------------------------------\nfrom langchain.chains.openai_functions.openapi import openapi_spec_to_openai_fn\nfrom langchain.utilities.openapi import OpenAPISpec\n# from langchain_community.tools import OpenAPISpec\ntext = \"\"\"\n{\n  \"openapi\": \"3.0.0\",\n  \"info\": {",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "text = \"\"\"\n{\n  \"openapi\": \"3.0.0\",\n  \"info\": {\n    \"version\": \"1.0.0\",\n    \"title\": \"Swagger Petstore\",\n    \"license\": {\n      \"name\": \"MIT\"\n    }\n  },",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "functions = [\n    format_tool_to_openai_function(f) for f in [\n        search_wikipedia, get_current_temperature\n    ]\n]\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(temperature=0).bind(functions=functions)\nresp_wtr = model.invoke(\"what is the weather in sf right now\")\nprint(f\"\\n\\n what is the weather in sf right now ? \\n {resp_wtr}\")\nresp_lc = model.invoke(\"what is langchain\")",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "model = ChatOpenAI(temperature=0).bind(functions=functions)\nresp_wtr = model.invoke(\"what is the weather in sf right now\")\nprint(f\"\\n\\n what is the weather in sf right now ? \\n {resp_wtr}\")\nresp_lc = model.invoke(\"what is langchain\")\nprint(f\"\\n\\n what is langchain? \\n {resp_lc}\")\n#----------------------------------------------------------------\nfrom langchain.prompts import ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "resp_wtr",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "resp_wtr = model.invoke(\"what is the weather in sf right now\")\nprint(f\"\\n\\n what is the weather in sf right now ? \\n {resp_wtr}\")\nresp_lc = model.invoke(\"what is langchain\")\nprint(f\"\\n\\n what is langchain? \\n {resp_lc}\")\n#----------------------------------------------------------------\nfrom langchain.prompts import ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),\n])",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "resp_lc",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "resp_lc = model.invoke(\"what is langchain\")\nprint(f\"\\n\\n what is langchain? \\n {resp_lc}\")\n#----------------------------------------------------------------\nfrom langchain.prompts import ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),\n])\nchain = prompt | model\nresp_wtr_2 = chain.invoke({\"input\": \"what is the weather in sf right now\"})",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),\n])\nchain = prompt | model\nresp_wtr_2 = chain.invoke({\"input\": \"what is the weather in sf right now\"})\nprint(f\"\\n\\n what is the weather in sf right now? \\n {resp_wtr_2}\")\n#----------------------------------------------------------------\nfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\nchain = prompt | model | OpenAIFunctionsAgentOutputParser()",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "chain = prompt | model\nresp_wtr_2 = chain.invoke({\"input\": \"what is the weather in sf right now\"})\nprint(f\"\\n\\n what is the weather in sf right now? \\n {resp_wtr_2}\")\n#----------------------------------------------------------------\nfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\nchain = prompt | model | OpenAIFunctionsAgentOutputParser()\nresult = chain.invoke({\"input\": \"what is the weather in sf right now\"})\nprint(f\"type(result) : {type(result)}\")\nprint(f\"result.tool : {result.tool}\")\nprint(f\"result.tool_input : {result.tool_input}\")",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "resp_wtr_2",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "resp_wtr_2 = chain.invoke({\"input\": \"what is the weather in sf right now\"})\nprint(f\"\\n\\n what is the weather in sf right now? \\n {resp_wtr_2}\")\n#----------------------------------------------------------------\nfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\nchain = prompt | model | OpenAIFunctionsAgentOutputParser()\nresult = chain.invoke({\"input\": \"what is the weather in sf right now\"})\nprint(f\"type(result) : {type(result)}\")\nprint(f\"result.tool : {result.tool}\")\nprint(f\"result.tool_input : {result.tool_input}\")\nget_current_temperature(result.tool_input)",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\nresult = chain.invoke({\"input\": \"what is the weather in sf right now\"})\nprint(f\"type(result) : {type(result)}\")\nprint(f\"result.tool : {result.tool}\")\nprint(f\"result.tool_input : {result.tool_input}\")\nget_current_temperature(result.tool_input)\nresult = chain.invoke({\"input\": \"hi!\"})\nprint(f\"type(result) : {type(result)}\")\nprint(f\"result.return_values : {result.return_values}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "result = chain.invoke({\"input\": \"what is the weather in sf right now\"})\nprint(f\"type(result) : {type(result)}\")\nprint(f\"result.tool : {result.tool}\")\nprint(f\"result.tool_input : {result.tool_input}\")\nget_current_temperature(result.tool_input)\nresult = chain.invoke({\"input\": \"hi!\"})\nprint(f\"type(result) : {type(result)}\")\nprint(f\"result.return_values : {result.return_values}\")\n#----------------------------------------------------------------\nfrom langchain.schema.agent import AgentFinish",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "result = chain.invoke({\"input\": \"hi!\"})\nprint(f\"type(result) : {type(result)}\")\nprint(f\"result.return_values : {result.return_values}\")\n#----------------------------------------------------------------\nfrom langchain.schema.agent import AgentFinish\ndef route(result):\n    if isinstance(result, AgentFinish):\n        return result.return_values['output']\n    else:\n        tools = {",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "chain = prompt | model | OpenAIFunctionsAgentOutputParser() | route\nresult_2 = chain.invoke({\"input\": \"What is the weather in san francisco right now?\"})\nprint(f\"\\n\\nWhat is the weather in san francisco right now?\\n {result_2}\")\nresult_3 = chain.invoke({\"input\": \"What is langchain?\"})\nprint(f\"\\n\\nWhat is langchain?\\n {result_3}\")\nresult_4 = chain.invoke({\"input\": \"hi!\"})\nprint(f\"\\n\\nhi!\\n {result_4}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "result_2",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "result_2 = chain.invoke({\"input\": \"What is the weather in san francisco right now?\"})\nprint(f\"\\n\\nWhat is the weather in san francisco right now?\\n {result_2}\")\nresult_3 = chain.invoke({\"input\": \"What is langchain?\"})\nprint(f\"\\n\\nWhat is langchain?\\n {result_3}\")\nresult_4 = chain.invoke({\"input\": \"hi!\"})\nprint(f\"\\n\\nhi!\\n {result_4}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "result_3",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "result_3 = chain.invoke({\"input\": \"What is langchain?\"})\nprint(f\"\\n\\nWhat is langchain?\\n {result_3}\")\nresult_4 = chain.invoke({\"input\": \"hi!\"})\nprint(f\"\\n\\nhi!\\n {result_4}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "result_4",
        "kind": 5,
        "importPath": "Functions_Langchain.L5-tools-routing-apis-student",
        "description": "Functions_Langchain.L5-tools-routing-apis-student",
        "peekOfCode": "result_4 = chain.invoke({\"input\": \"hi!\"})\nprint(f\"\\n\\nhi!\\n {result_4}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L5-tools-routing-apis-student",
        "documentation": {}
    },
    {
        "label": "OpenMeteoInput",
        "kind": 6,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "class OpenMeteoInput(BaseModel):\n    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n@tool(args_schema=OpenMeteoInput)\ndef get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {\n        'latitude': latitude,",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "cbfs",
        "kind": 6,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "class cbfs(param.Parameterized):\n    def __init__(self, tools, **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []\n        self.functions = [format_tool_to_openai_function(f) for f in tools]\n        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n        self.prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are helpful but sassy assistant\"),\n            MessagesPlaceholder(variable_name=\"chat_history\"),",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "get_current_temperature",
        "kind": 2,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "def get_current_temperature(latitude: float, longitude: float) -> dict:\n    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n    # Parameters for the request\n    params = {\n        'latitude': latitude,\n        'longitude': longitude,\n        'hourly': 'temperature_2m',\n        'forecast_days': 1,\n    }",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "search_wikipedia",
        "kind": 2,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "def search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n    for page_title in page_titles[: 3]:\n        try:\n            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n        except (\n            self.wiki_client.exceptions.PageError,",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "run_agent",
        "kind": 2,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "def run_agent(user_input):\n    intermediate_steps = []\n    while True:\n        result = chain.invoke({\n            \"input\": user_input, \n            \"agent_scratchpad\": format_to_openai_functions(intermediate_steps)\n        })\n        if isinstance(result, AgentFinish):\n            return result\n        tool = {",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "run_agent",
        "kind": 2,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "def run_agent(user_input):\n    intermediate_steps = []\n    while True:\n        result = agent_chain.invoke({\n            \"input\": user_input, \n            \"intermediate_steps\": intermediate_steps\n        })\n        if isinstance(result, AgentFinish):\n            return result\n        tool = {",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "create_your_own",
        "kind": 2,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "def create_your_own(query: str) -> str:\n    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n    print(type(query))\n    return query[::-1]\ntools = [get_current_temperature, search_wikipedia, create_your_own]\nimport panel as pn  # GUI\npn.extension()\n# import panel as pn\nimport param\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "tools = [get_current_temperature, search_wikipedia]\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.tools.render import format_tool_to_openai_function\nfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\nfunctions = [format_tool_to_openai_function(f) for f in tools]\nmodel = ChatOpenAI(temperature=0).bind(functions=functions)\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "functions",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "functions = [format_tool_to_openai_function(f) for f in tools]\nmodel = ChatOpenAI(temperature=0).bind(functions=functions)\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),\n])\nchain = prompt | model | OpenAIFunctionsAgentOutputParser()\nresult_wtr_1 = chain.invoke({\"input\": \"what is the weather is sf?\"})\nprint(f\"\\n\\nwhat is the weather is sf?\\n{result_wtr_1}\")\nprint(f\"\\n\\nresult.tool : \\n{result_wtr_1.tool}\")",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "model = ChatOpenAI(temperature=0).bind(functions=functions)\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),\n])\nchain = prompt | model | OpenAIFunctionsAgentOutputParser()\nresult_wtr_1 = chain.invoke({\"input\": \"what is the weather is sf?\"})\nprint(f\"\\n\\nwhat is the weather is sf?\\n{result_wtr_1}\")\nprint(f\"\\n\\nresult.tool : \\n{result_wtr_1.tool}\")\nprint(f\"\\n\\nresult.tool_input?\\n{result_wtr_1.tool_input}\")",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),\n])\nchain = prompt | model | OpenAIFunctionsAgentOutputParser()\nresult_wtr_1 = chain.invoke({\"input\": \"what is the weather is sf?\"})\nprint(f\"\\n\\nwhat is the weather is sf?\\n{result_wtr_1}\")\nprint(f\"\\n\\nresult.tool : \\n{result_wtr_1.tool}\")\nprint(f\"\\n\\nresult.tool_input?\\n{result_wtr_1.tool_input}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\nresult_wtr_1 = chain.invoke({\"input\": \"what is the weather is sf?\"})\nprint(f\"\\n\\nwhat is the weather is sf?\\n{result_wtr_1}\")\nprint(f\"\\n\\nresult.tool : \\n{result_wtr_1.tool}\")\nprint(f\"\\n\\nresult.tool_input?\\n{result_wtr_1.tool_input}\")\n#----------------------------------------------------------------\nfrom langchain.prompts import MessagesPlaceholder\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "result_wtr_1",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "result_wtr_1 = chain.invoke({\"input\": \"what is the weather is sf?\"})\nprint(f\"\\n\\nwhat is the weather is sf?\\n{result_wtr_1}\")\nprint(f\"\\n\\nresult.tool : \\n{result_wtr_1.tool}\")\nprint(f\"\\n\\nresult.tool_input?\\n{result_wtr_1.tool_input}\")\n#----------------------------------------------------------------\nfrom langchain.prompts import MessagesPlaceholder\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\")",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    (\"user\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n])\nchain = prompt | model | OpenAIFunctionsAgentOutputParser()\nresult1 = chain.invoke({\n    \"input\": \"what is the weather is sf?\",\n    \"agent_scratchpad\": []\n})",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\nresult1 = chain.invoke({\n    \"input\": \"what is the weather is sf?\",\n    \"agent_scratchpad\": []\n})\nprint(f\"\\n\\nresult1.tool : \\n{result1.tool}\")\nobservation = get_current_temperature(result1.tool_input)\nprint(f\"observation : \\n {observation}\")\nprint(f\"/n/ntype(result1)/n : {type(result1)}\")\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "result1",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "result1 = chain.invoke({\n    \"input\": \"what is the weather is sf?\",\n    \"agent_scratchpad\": []\n})\nprint(f\"\\n\\nresult1.tool : \\n{result1.tool}\")\nobservation = get_current_temperature(result1.tool_input)\nprint(f\"observation : \\n {observation}\")\nprint(f\"/n/ntype(result1)/n : {type(result1)}\")\n#----------------------------------------------------------------\nfrom langchain.agents.format_scratchpad import format_to_openai_functions",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "observation",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "observation = get_current_temperature(result1.tool_input)\nprint(f\"observation : \\n {observation}\")\nprint(f\"/n/ntype(result1)/n : {type(result1)}\")\n#----------------------------------------------------------------\nfrom langchain.agents.format_scratchpad import format_to_openai_functions\nprint(f\"\\n\\nresult1.message_log : \\n{result1.message_log}\")\nformat_to_openai_functions([(result1, observation), ])\nresult2 = chain.invoke({\n    \"input\": \"what is the weather is sf?\", \n    \"agent_scratchpad\": format_to_openai_functions([(result1, observation)])",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "result2",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "result2 = chain.invoke({\n    \"input\": \"what is the weather is sf?\", \n    \"agent_scratchpad\": format_to_openai_functions([(result1, observation)])\n})\nprint(f\"\\n\\nresult2 : \\n {result2}\")\n#----------------------------------------------------------------\nfrom langchain.schema.agent import AgentFinish\ndef run_agent(user_input):\n    intermediate_steps = []\n    while True:",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "agent_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "agent_chain = RunnablePassthrough.assign(\n    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n) | chain\ndef run_agent(user_input):\n    intermediate_steps = []\n    while True:\n        result = agent_chain.invoke({\n            \"input\": user_input, \n            \"intermediate_steps\": intermediate_steps\n        })",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "resp_wtr_3",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "resp_wtr_3 = run_agent(\"what is the weather is sf?\")\nprint(f\"\\n\\nwhat is the weather is sf?\\n{resp_wtr_3}\")\nresp_lc_3 = run_agent(\"what is langchain?\")\nprint(f\"\\n\\nwhat is langchain?\\n{resp_lc_3}\")\nresp_hi_3 = run_agent(\"hi!\")\nprint(f\"\\n\\nhi!\\n{resp_hi_3}\")\n#----------------------------------------------------------------\nfrom langchain.agents import AgentExecutor\nagent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"what is langchain?\"})}\")",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "resp_lc_3",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "resp_lc_3 = run_agent(\"what is langchain?\")\nprint(f\"\\n\\nwhat is langchain?\\n{resp_lc_3}\")\nresp_hi_3 = run_agent(\"hi!\")\nprint(f\"\\n\\nhi!\\n{resp_hi_3}\")\n#----------------------------------------------------------------\nfrom langchain.agents import AgentExecutor\nagent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"what is langchain?\"})}\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"my name is bob\"})}\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"what is my name\"})}\")",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "resp_hi_3",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "resp_hi_3 = run_agent(\"hi!\")\nprint(f\"\\n\\nhi!\\n{resp_hi_3}\")\n#----------------------------------------------------------------\nfrom langchain.agents import AgentExecutor\nagent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"what is langchain?\"})}\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"my name is bob\"})}\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"what is my name\"})}\")\n#----------------------------------------------------------------\nprompt = ChatPromptTemplate.from_messages([",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"what is langchain?\"})}\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"my name is bob\"})}\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"what is my name\"})}\")\n#----------------------------------------------------------------\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"user\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\")",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful but sassy assistant\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"user\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n])\nagent_chain = RunnablePassthrough.assign(\n    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n) | prompt | model | OpenAIFunctionsAgentOutputParser()\nfrom langchain.memory import ConversationBufferMemory",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "agent_chain",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "agent_chain = RunnablePassthrough.assign(\n    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n) | prompt | model | OpenAIFunctionsAgentOutputParser()\nfrom langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\nagent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"whats the weather in sf?\"})}\")\nprint(\"---------------------------------------------\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"my name is bob\"})}\")\nprint(\"---------------------------------------------\")",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\nagent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"whats the weather in sf?\"})}\")\nprint(\"---------------------------------------------\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"my name is bob\"})}\")\nprint(\"---------------------------------------------\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"what is my name\"})}\")\nprint(\"---------------------------------------------\")\n#----------------------------------------------------------------\n# Create a chatbot",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"whats the weather in sf?\"})}\")\nprint(\"---------------------------------------------\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"my name is bob\"})}\")\nprint(\"---------------------------------------------\")\nprint(f\"\\n\\n{agent_executor.invoke({\"input\": \"what is my name\"})}\")\nprint(\"---------------------------------------------\")\n#----------------------------------------------------------------\n# Create a chatbot\n@tool",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "tools = [get_current_temperature, search_wikipedia, create_your_own]\nimport panel as pn  # GUI\npn.extension()\n# import panel as pn\nimport param\n#----------------------------------------------------------------\nclass cbfs(param.Parameterized):\n    def __init__(self, tools, **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "cb",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "cb = cbfs(tools)\ninp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "inp = pn.widgets.TextInput( placeholder='Enter your query here …')\nconversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "conversation = pn.bind(cb.convchain, inp) \ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "tab1",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "tab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=400),\n    pn.layout.Divider(),\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "dashboard",
        "kind": 5,
        "importPath": "Functions_Langchain.L6-functional_conversation-student",
        "description": "Functions_Langchain.L6-functional_conversation-student",
        "peekOfCode": "dashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('Q&A Chat-Bot')),\n    pn.Tabs(('Conversation', tab1))\n)\n# print(dashboard)\n# Start the Panel server to render the chatbot dashboard\ndashboard.show(port=5006)  # You can specify any port number you prefer\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.L6-functional_conversation-student",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Functions_Langchain.testing",
        "description": "Functions_Langchain.testing",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\nresp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})",
        "detail": "Functions_Langchain.testing",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Functions_Langchain.testing",
        "description": "Functions_Langchain.testing",
        "peekOfCode": "model = ChatOpenAI()\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\nresp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})\n    print(\"\\nResponse from async call : \", resp_invoke_async)\nimport asyncio\nasyncio.run(async_call())",
        "detail": "Functions_Langchain.testing",
        "documentation": {}
    },
    {
        "label": "output_parser",
        "kind": 5,
        "importPath": "Functions_Langchain.testing",
        "description": "Functions_Langchain.testing",
        "peekOfCode": "output_parser = StrOutputParser()\nchain = prompt | model | output_parser\nresp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})\n    print(\"\\nResponse from async call : \", resp_invoke_async)\nimport asyncio\nasyncio.run(async_call())\n#----------------------------------------------------------------",
        "detail": "Functions_Langchain.testing",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "Functions_Langchain.testing",
        "description": "Functions_Langchain.testing",
        "peekOfCode": "chain = prompt | model | output_parser\nresp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})\n    print(\"\\nResponse from async call : \", resp_invoke_async)\nimport asyncio\nasyncio.run(async_call())\n#----------------------------------------------------------------\nresp_chain_batch_sync = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])",
        "detail": "Functions_Langchain.testing",
        "documentation": {}
    },
    {
        "label": "resp_invoke_sync",
        "kind": 5,
        "importPath": "Functions_Langchain.testing",
        "description": "Functions_Langchain.testing",
        "peekOfCode": "resp_invoke_sync = chain.invoke({\"topic\": \"bears\"})\nprint(\"\\nResponse from sync call : \", resp_invoke_sync)\nasync def async_call():\n    resp_invoke_async = await chain.ainvoke({\"topic\": \"bears\"})\n    print(\"\\nResponse from async call : \", resp_invoke_async)\nimport asyncio\nasyncio.run(async_call())\n#----------------------------------------------------------------\nresp_chain_batch_sync = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])\nprint(\"\\n\\nResponse from sync batch call : \\n\", resp_chain_batch_sync)",
        "detail": "Functions_Langchain.testing",
        "documentation": {}
    },
    {
        "label": "resp_chain_batch_sync",
        "kind": 5,
        "importPath": "Functions_Langchain.testing",
        "description": "Functions_Langchain.testing",
        "peekOfCode": "resp_chain_batch_sync = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])\nprint(\"\\n\\nResponse from sync batch call : \\n\", resp_chain_batch_sync)\nfor t in chain.stream({\"topic\": \"bears\"}):\n    print(t)\n# response = await chain.ainvoke({\"topic\": \"bears\"})\n# response",
        "detail": "Functions_Langchain.testing",
        "documentation": {}
    },
    {
        "label": "welcome",
        "kind": 2,
        "importPath": "Functions_Langchain.testing_fastapi",
        "description": "Functions_Langchain.testing_fastapi",
        "peekOfCode": "def welcome():\n    return {\"message\": \"Hello FastAPI\"}\n@app.get(\"/home\")\ndef welcome_home():\n    return {\"message\": \"Hello FastAPI Home\"}\n@app.post(\"/dummy\")\ndef welcome_dummy(data):\n    return {\"message\": data}\nif __name__ == \"__main__\":\n    uvicorn.run(app=app, host=\"0.0.0.0\", port=80)",
        "detail": "Functions_Langchain.testing_fastapi",
        "documentation": {}
    },
    {
        "label": "welcome_home",
        "kind": 2,
        "importPath": "Functions_Langchain.testing_fastapi",
        "description": "Functions_Langchain.testing_fastapi",
        "peekOfCode": "def welcome_home():\n    return {\"message\": \"Hello FastAPI Home\"}\n@app.post(\"/dummy\")\ndef welcome_dummy(data):\n    return {\"message\": data}\nif __name__ == \"__main__\":\n    uvicorn.run(app=app, host=\"0.0.0.0\", port=80)\n# uvicorn Functions_Langchain.testing_fastapi:app --reload",
        "detail": "Functions_Langchain.testing_fastapi",
        "documentation": {}
    },
    {
        "label": "welcome_dummy",
        "kind": 2,
        "importPath": "Functions_Langchain.testing_fastapi",
        "description": "Functions_Langchain.testing_fastapi",
        "peekOfCode": "def welcome_dummy(data):\n    return {\"message\": data}\nif __name__ == \"__main__\":\n    uvicorn.run(app=app, host=\"0.0.0.0\", port=80)\n# uvicorn Functions_Langchain.testing_fastapi:app --reload",
        "detail": "Functions_Langchain.testing_fastapi",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Functions_Langchain.testing_fastapi",
        "description": "Functions_Langchain.testing_fastapi",
        "peekOfCode": "app = FastAPI()\n@app.get(\"/\")   #get, post, put, delete\ndef welcome():\n    return {\"message\": \"Hello FastAPI\"}\n@app.get(\"/home\")\ndef welcome_home():\n    return {\"message\": \"Hello FastAPI Home\"}\n@app.post(\"/dummy\")\ndef welcome_dummy(data):\n    return {\"message\": data}",
        "detail": "Functions_Langchain.testing_fastapi",
        "documentation": {}
    },
    {
        "label": "is_not_toxic",
        "kind": 2,
        "importPath": "Functions_Langchain.waylabs_waylogs_langkit",
        "description": "Functions_Langchain.waylabs_waylogs_langkit",
        "peekOfCode": "def is_not_toxic(prompt_message):\n    profile = why.log({\"prompt\": prompt_message}, schema=text_metrics_schema).profile().view()\n    profile_dict = profile.get_column('prompt.toxicity').to_summary_dict()\n    tox_max = profile_dict['distribution/max']\n    print(f'Toxic score: {tox_max}')\n    if tox_max > 0.5:\n      return False\n    else:\n      return True\nprint(f\"Is 'Do you like fruit?' toxic : {is_not_toxic(\"Do you like fruit?\")}\")",
        "detail": "Functions_Langchain.waylabs_waylogs_langkit",
        "documentation": {}
    },
    {
        "label": "text_metrics_schema",
        "kind": 5,
        "importPath": "Functions_Langchain.waylabs_waylogs_langkit",
        "description": "Functions_Langchain.waylabs_waylogs_langkit",
        "peekOfCode": "text_metrics_schema = llm_metrics.init()\n# Set to show all columns in dataframe\npd.set_option(\"display.max_columns\", None)\nprint(\"downloading models and initialized metrics...\")\ntext_metrics_schema = llm_metrics.init()\ndef is_not_toxic(prompt_message):\n    profile = why.log({\"prompt\": prompt_message}, schema=text_metrics_schema).profile().view()\n    profile_dict = profile.get_column('prompt.toxicity').to_summary_dict()\n    tox_max = profile_dict['distribution/max']\n    print(f'Toxic score: {tox_max}')",
        "detail": "Functions_Langchain.waylabs_waylogs_langkit",
        "documentation": {}
    },
    {
        "label": "text_metrics_schema",
        "kind": 5,
        "importPath": "Functions_Langchain.waylabs_waylogs_langkit",
        "description": "Functions_Langchain.waylabs_waylogs_langkit",
        "peekOfCode": "text_metrics_schema = llm_metrics.init()\ndef is_not_toxic(prompt_message):\n    profile = why.log({\"prompt\": prompt_message}, schema=text_metrics_schema).profile().view()\n    profile_dict = profile.get_column('prompt.toxicity').to_summary_dict()\n    tox_max = profile_dict['distribution/max']\n    print(f'Toxic score: {tox_max}')\n    if tox_max > 0.5:\n      return False\n    else:\n      return True",
        "detail": "Functions_Langchain.waylabs_waylogs_langkit",
        "documentation": {}
    },
    {
        "label": "Tee",
        "kind": 6,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "class Tee:\n    def __init__(self, file):\n        self.f = file\n    def write(self, what):\n        if self.f is not None:\n            try:\n                self.f.write(what.replace(\"\\n\", \"\\r\\n\"))\n            except OSError:\n                pass\n        tee_f.write(what)",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "CopyTo",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def CopyTo(desc, src, dest):\n    import win32api\n    import win32con\n    while 1:\n        try:\n            win32api.CopyFile(src, dest, 0)\n            return\n        except win32api.error as details:\n            if details.winerror == 5:  # access denied - user not admin.\n                raise",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "LoadSystemModule",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def LoadSystemModule(lib_dir, modname):\n    # See if this is a debug build.\n    import importlib.machinery\n    import importlib.util\n    suffix = \"_d\" if \"_d.pyd\" in importlib.machinery.EXTENSION_SUFFIXES else \"\"\n    filename = \"%s%d%d%s.dll\" % (\n        modname,\n        sys.version_info.major,\n        sys.version_info.minor,\n        suffix,",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "SetPyKeyVal",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def SetPyKeyVal(key_name, value_name, value):\n    root_hkey = get_root_hkey()\n    root_key = winreg.OpenKey(root_hkey, root_key_name)\n    try:\n        my_key = winreg.CreateKey(root_key, key_name)\n        try:\n            winreg.SetValueEx(my_key, value_name, 0, winreg.REG_SZ, value)\n            if verbose:\n                print(f\"-> {root_key_name}\\\\{key_name}[{value_name}]={value!r}\")\n        finally:",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "UnsetPyKeyVal",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def UnsetPyKeyVal(key_name, value_name, delete_key=False):\n    root_hkey = get_root_hkey()\n    root_key = winreg.OpenKey(root_hkey, root_key_name)\n    try:\n        my_key = winreg.OpenKey(root_key, key_name, 0, winreg.KEY_SET_VALUE)\n        try:\n            winreg.DeleteValue(my_key, value_name)\n            if verbose:\n                print(f\"-> DELETE {root_key_name}\\\\{key_name}[{value_name}]\")\n        finally:",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "RegisterCOMObjects",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def RegisterCOMObjects(register=True):\n    import win32com.server.register\n    if register:\n        func = win32com.server.register.RegisterClasses\n    else:\n        func = win32com.server.register.UnregisterClasses\n    flags = {}\n    if not verbose:\n        flags[\"quiet\"] = 1\n    for module, klass_name in com_modules:",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "RegisterHelpFile",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def RegisterHelpFile(register=True, lib_dir=None):\n    if lib_dir is None:\n        lib_dir = sysconfig.get_paths()[\"platlib\"]\n    if register:\n        # Register the .chm help file.\n        chm_file = os.path.join(lib_dir, \"PyWin32.chm\")\n        if os.path.isfile(chm_file):\n            # This isn't recursive, so if 'Help' doesn't exist, we croak\n            SetPyKeyVal(\"Help\", None, None)\n            SetPyKeyVal(\"Help\\\\Pythonwin Reference\", None, chm_file)",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "RegisterPythonwin",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def RegisterPythonwin(register=True, lib_dir=None):\n    \"\"\"Add (or remove) Pythonwin to context menu for python scripts.\n    ??? Should probably also add Edit command for pys files also.\n    Also need to remove these keys on uninstall, but there's no function\n        like file_created to add registry entries to uninstall log ???\n    \"\"\"\n    import os\n    if lib_dir is None:\n        lib_dir = sysconfig.get_paths()[\"platlib\"]\n    classes_root = get_root_hkey()",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "get_shortcuts_folder",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def get_shortcuts_folder():\n    if get_root_hkey() == winreg.HKEY_LOCAL_MACHINE:\n        try:\n            fldr = get_special_folder_path(\"CSIDL_COMMON_PROGRAMS\")\n        except OSError:\n            # No CSIDL_COMMON_PROGRAMS on this platform\n            fldr = get_special_folder_path(\"CSIDL_PROGRAMS\")\n    else:\n        # non-admin install - always goes in this user's start menu.\n        fldr = get_special_folder_path(\"CSIDL_PROGRAMS\")",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "get_system_dir",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def get_system_dir():\n    import win32api  # we assume this exists.\n    try:\n        import pythoncom\n        import win32process\n        from win32com.shell import shell, shellcon\n        try:\n            if win32process.IsWow64Process():\n                return shell.SHGetSpecialFolderPath(0, shellcon.CSIDL_SYSTEMX86)\n            return shell.SHGetSpecialFolderPath(0, shellcon.CSIDL_SYSTEM)",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "fixup_dbi",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def fixup_dbi():\n    # We used to have a dbi.pyd with our .pyd files, but now have a .py file.\n    # If the user didn't uninstall, they will find the .pyd which will cause\n    # problems - so handle that.\n    import win32api\n    import win32con\n    pyd_name = os.path.join(os.path.dirname(win32api.__file__), \"dbi.pyd\")\n    pyd_d_name = os.path.join(os.path.dirname(win32api.__file__), \"dbi_d.pyd\")\n    py_name = os.path.join(os.path.dirname(win32con.__file__), \"dbi.py\")\n    for this_pyd in (pyd_name, pyd_d_name):",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "install",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def install(lib_dir):\n    import traceback\n    # The .pth file is now installed as a regular file.\n    # Create the .pth file in the site-packages dir, and use only relative paths\n    # We used to write a .pth directly to sys.prefix - clobber it.\n    if os.path.isfile(os.path.join(sys.prefix, \"pywin32.pth\")):\n        os.unlink(os.path.join(sys.prefix, \"pywin32.pth\"))\n    # The .pth may be new and therefore not loaded in this session.\n    # Setup the paths just in case.\n    for name in \"win32 win32\\\\lib Pythonwin\".split():",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "uninstall",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def uninstall(lib_dir):\n    # First ensure our system modules are loaded from pywin32_system, so\n    # we can remove the ones we copied...\n    LoadSystemModule(lib_dir, \"pywintypes\")\n    LoadSystemModule(lib_dir, \"pythoncom\")\n    try:\n        RegisterCOMObjects(False)\n    except Exception as why:\n        print(f\"Failed to unregister COM objects: {why}\")\n    try:",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "verify_destination",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def verify_destination(location):\n    if not os.path.isdir(location):\n        raise argparse.ArgumentTypeError(f'Path \"{location}\" does not exist!')\n    return location\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=\"\"\"A post-install script for the pywin32 extensions.\n    * Typical usage:\n    > python pywin32_postinstall.py -install",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=\"\"\"A post-install script for the pywin32 extensions.\n    * Typical usage:\n    > python pywin32_postinstall.py -install\n    If you installed pywin32 via a .exe installer, this should be run\n    automatically after installation, but if it fails you can run it again.\n    If you installed pywin32 via PIP, you almost certainly need to run this to\n    setup the environment correctly.",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "tee_f",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "tee_f = open(os.path.join(tempfile.gettempdir(), \"pywin32_postinstall.log\"), \"w\")\nclass Tee:\n    def __init__(self, file):\n        self.f = file\n    def write(self, what):\n        if self.f is not None:\n            try:\n                self.f.write(what.replace(\"\\n\", \"\\r\\n\"))\n            except OSError:\n                pass",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "sys.stderr",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "sys.stderr = Tee(sys.stderr)\nsys.stdout = Tee(sys.stdout)\ncom_modules = [\n    # module_name,                      class_names\n    (\"win32com.servers.interp\", \"Interpreter\"),\n    (\"win32com.servers.dictionary\", \"DictionaryPolicy\"),\n    (\"win32com.axscript.client.pyscript\", \"PyScript\"),\n]\n# Is this a 'silent' install - ie, avoid all dialogs.\n# Different than 'verbose'",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "sys.stdout = Tee(sys.stdout)\ncom_modules = [\n    # module_name,                      class_names\n    (\"win32com.servers.interp\", \"Interpreter\"),\n    (\"win32com.servers.dictionary\", \"DictionaryPolicy\"),\n    (\"win32com.axscript.client.pyscript\", \"PyScript\"),\n]\n# Is this a 'silent' install - ie, avoid all dialogs.\n# Different than 'verbose'\nsilent = 0",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "com_modules",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "com_modules = [\n    # module_name,                      class_names\n    (\"win32com.servers.interp\", \"Interpreter\"),\n    (\"win32com.servers.dictionary\", \"DictionaryPolicy\"),\n    (\"win32com.axscript.client.pyscript\", \"PyScript\"),\n]\n# Is this a 'silent' install - ie, avoid all dialogs.\n# Different than 'verbose'\nsilent = 0\n# Verbosity of output messages.",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "silent",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "silent = 0\n# Verbosity of output messages.\nverbose = 1\nroot_key_name = \"Software\\\\Python\\\\PythonCore\\\\\" + sys.winver\ntry:\n    # When this script is run from inside the bdist_wininst installer,\n    # file_created() and directory_created() are additional builtin\n    # functions which write lines to PythonXX\\pywin32-install.log. This is\n    # a list of actions for the uninstaller, the format is inspired by what\n    # the Wise installer also creates.",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "verbose",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "verbose = 1\nroot_key_name = \"Software\\\\Python\\\\PythonCore\\\\\" + sys.winver\ntry:\n    # When this script is run from inside the bdist_wininst installer,\n    # file_created() and directory_created() are additional builtin\n    # functions which write lines to PythonXX\\pywin32-install.log. This is\n    # a list of actions for the uninstaller, the format is inspired by what\n    # the Wise installer also creates.\n    file_created  # type: ignore[used-before-def]\n    # 3.10 stopped supporting bdist_wininst, but we can still build them with 3.9.",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "root_key_name",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_postinstall",
        "description": "react_venv.Scripts.pywin32_postinstall",
        "peekOfCode": "root_key_name = \"Software\\\\Python\\\\PythonCore\\\\\" + sys.winver\ntry:\n    # When this script is run from inside the bdist_wininst installer,\n    # file_created() and directory_created() are additional builtin\n    # functions which write lines to PythonXX\\pywin32-install.log. This is\n    # a list of actions for the uninstaller, the format is inspired by what\n    # the Wise installer also creates.\n    file_created  # type: ignore[used-before-def]\n    # 3.10 stopped supporting bdist_wininst, but we can still build them with 3.9.\n    # This can be kept until Python 3.9 or exe installers support is dropped.",
        "detail": "react_venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "run_test",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_testall",
        "description": "react_venv.Scripts.pywin32_testall",
        "peekOfCode": "def run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.\n    cmd = [sys.executable, \"-u\", scriptname] + cmdline_extras\n    print(\"--- Running '%s' ---\" % script)\n    sys.stdout.flush()\n    result = subprocess.run(cmd, check=False, cwd=dirname)\n    print(f\"*** Test script '{script}' exited with {result.returncode}\")\n    sys.stdout.flush()\n    if result.returncode:",
        "detail": "react_venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "find_and_run",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_testall",
        "description": "react_venv.Scripts.pywin32_testall",
        "peekOfCode": "def find_and_run(possible_locations, extras):\n    for maybe in possible_locations:\n        if os.path.isfile(maybe):\n            run_test(maybe, extras)\n            break\n    else:\n        raise RuntimeError(\n            \"Failed to locate a test script in one of %s\" % possible_locations\n        )\ndef main():",
        "detail": "react_venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "react_venv.Scripts.pywin32_testall",
        "description": "react_venv.Scripts.pywin32_testall",
        "peekOfCode": "def main():\n    import argparse\n    code_directories = [this_dir] + site_packages\n    parser = argparse.ArgumentParser(\n        description=\"A script to trigger tests in all subprojects of PyWin32.\"\n    )\n    parser.add_argument(\n        \"-no-user-interaction\",\n        default=False,\n        action=\"store_true\",",
        "detail": "react_venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "this_dir",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_testall",
        "description": "react_venv.Scripts.pywin32_testall",
        "peekOfCode": "this_dir = os.path.dirname(__file__)\nsite_packages = [\n    site.getusersitepackages(),\n] + site.getsitepackages()\nfailures = []\n# Run a test using subprocess and wait for the result.\n# If we get an returncode != 0, we know that there was an error, but we don't\n# abort immediately - we run as many tests as we can.\ndef run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)",
        "detail": "react_venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "site_packages",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_testall",
        "description": "react_venv.Scripts.pywin32_testall",
        "peekOfCode": "site_packages = [\n    site.getusersitepackages(),\n] + site.getsitepackages()\nfailures = []\n# Run a test using subprocess and wait for the result.\n# If we get an returncode != 0, we know that there was an error, but we don't\n# abort immediately - we run as many tests as we can.\ndef run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.",
        "detail": "react_venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "failures",
        "kind": 5,
        "importPath": "react_venv.Scripts.pywin32_testall",
        "description": "react_venv.Scripts.pywin32_testall",
        "peekOfCode": "failures = []\n# Run a test using subprocess and wait for the result.\n# If we get an returncode != 0, we know that there was an error, but we don't\n# abort immediately - we run as many tests as we can.\ndef run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.\n    cmd = [sys.executable, \"-u\", scriptname] + cmdline_extras\n    print(\"--- Running '%s' ---\" % script)\n    sys.stdout.flush()",
        "detail": "react_venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "AgentCallbackHandler",
        "kind": 6,
        "importPath": "callbacks",
        "description": "callbacks",
        "peekOfCode": "class AgentCallbackHandler(BaseCallbackHandler):\n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when LLM starts running.\"\"\"\n        print(f\"***Prompt to LLM was:***\\n{prompts[0]}\")\n        print(\"*********\")\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n        \"\"\"Run when LLM ends running.\"\"\"\n        print(f\"***LLM Response:***\\n{response.generations[0][0].text}\")",
        "detail": "callbacks",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "csv_agent",
        "description": "csv_agent",
        "peekOfCode": "def main():\n    print(\"Starting for CSV Agent ...\")\n    instr_python_agent = \"\"\"\n    You are a agent to write & execute code in python to answer questions.\n    You have access to Python REPL that you use to execute python code.\n    You have qrcode package installed.\n    If you have any error, debug & try again to fix the error.\n    Only use output of your code to answer the question.\n    You might know the answer without running any code, but you should run the code to get answer.\n    If it seems like you can't write code to answer the question, just return \"I don't know\" as answer.",
        "detail": "csv_agent",
        "documentation": {}
    },
    {
        "label": "multiply",
        "kind": 2,
        "importPath": "function_calling",
        "description": "function_calling",
        "peekOfCode": "def multiply(a: float, b: float)->float:\n    \"\"\"Multiply 'a' times 'b'.\"\"\"\n    return a * b\nif __name__ == '__main__':\n    print(\"Starting Function Calling ...\")\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", \"You are a hekpful assistant\"),\n            (\"human\", \"{input}\"),\n            (\"placeholder\", \"{agent_scratchpad}\"),",
        "detail": "function_calling",
        "documentation": {}
    },
    {
        "label": "get_text_length",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def get_text_length(text: str)-> int:\n    \"\"\"\n    Returns the length of the given text by characters.\n    \"\"\"\n    print(f\"get_text_length entered with {text}\")\n    text = text.strip(\"'\\n\").strip('\"')\n    return len(text)\ndef find_tool_by_name(tools: List[Tool], tool_name: str) -> Tool:\n    for tool in tools:\n        if tool.name == tool_name:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "find_tool_by_name",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def find_tool_by_name(tools: List[Tool], tool_name: str) -> Tool:\n    for tool in tools:\n        if tool.name == tool_name:\n            return tool\n    raise ValueError(f\"Tool '{tool_name}' not found in the provided list.\")\nif __name__ == '__main__':\n    print(\"Hello ReAct Langchain\")\n    # print(f\"Length of the Text is : {get_text_length(text='Hello ReAct Langchain')}\")\n    tools = [get_text_length]\n    template = \"\"\"",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "openai_api_key",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n@tool\ndef get_text_length(text: str)-> int:\n    \"\"\"\n    Returns the length of the given text by characters.\n    \"\"\"\n    print(f\"get_text_length entered with {text}\")\n    text = text.strip(\"'\\n\").strip('\"')\n    return len(text)\ndef find_tool_by_name(tools: List[Tool], tool_name: str) -> Tool:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "python_agent",
        "description": "python_agent",
        "peekOfCode": "def main():\n    print(\"Starting ...\")\n    instr_python_agent = \"\"\"\n    You are a agent to write & execute code in python to answer questions.\n    You have access to Python REPL that you use to execute python code.\n    You have qrcode package installed.\n    If you have any error, debug & try again to fix the error.\n    Only use output of your code to answer the question.\n    You might know the answer without running any code, but you should run the code to get answer.\n    If it seems like you can't write code to answer the question, just return \"I don't know\" as answer.",
        "detail": "python_agent",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "router_agent",
        "description": "router_agent",
        "peekOfCode": "def main():\n    print(\"Starting ...\")\n    instr_python_agent = \"\"\"\n    You are a agent to write & execute code in python to answer questions.\n    You have access to Python REPL that you use to execute python code.\n    You have qrcode package installed.\n    If you have any error, debug & try again to fix the error.\n    Only use output of your code to answer the question.\n    You might know the answer without running any code, but you should run the code to get answer.\n    If it seems like you can't write code to answer the question, just return \"I don't know\" as answer.",
        "detail": "router_agent",
        "documentation": {}
    },
    {
        "label": "welcome",
        "kind": 2,
        "importPath": "test_fastapi",
        "description": "test_fastapi",
        "peekOfCode": "def welcome():\n    return {\"message\": \"Hello FastAPI\"}\n@app.get(\"/home\")\ndef welcome_home():\n    return {\"message\": \"Hello FastAPI Home\"}\n@app.post(\"/dummy\")\ndef welcome_dummy(data):\n    return {\"message\": data}",
        "detail": "test_fastapi",
        "documentation": {}
    },
    {
        "label": "welcome_home",
        "kind": 2,
        "importPath": "test_fastapi",
        "description": "test_fastapi",
        "peekOfCode": "def welcome_home():\n    return {\"message\": \"Hello FastAPI Home\"}\n@app.post(\"/dummy\")\ndef welcome_dummy(data):\n    return {\"message\": data}",
        "detail": "test_fastapi",
        "documentation": {}
    },
    {
        "label": "welcome_dummy",
        "kind": 2,
        "importPath": "test_fastapi",
        "description": "test_fastapi",
        "peekOfCode": "def welcome_dummy(data):\n    return {\"message\": data}",
        "detail": "test_fastapi",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "test_fastapi",
        "description": "test_fastapi",
        "peekOfCode": "app = FastAPI()\n@app.get(\"/\")   #get, post, put, delete\ndef welcome():\n    return {\"message\": \"Hello FastAPI\"}\n@app.get(\"/home\")\ndef welcome_home():\n    return {\"message\": \"Hello FastAPI Home\"}\n@app.post(\"/dummy\")\ndef welcome_dummy(data):\n    return {\"message\": data}",
        "detail": "test_fastapi",
        "documentation": {}
    }
]